#!/usr/bin/env python
import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    with open(os.path.join(partial_path, "__init__.py"), "w") as f:
                        f.write("\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "w") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('beta.py', "\nimport pandas as pd\nimport numpy as np\nimport json\nfrom random import randrange\n    \n\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n\n        \nclass rps(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def rps(self, history):\n        return self.shift % 3\n    \n\nagents = {    \n    'rps_0': rps(0),\n    'rps_1': rps(1),\n    'rps_2': rps(2),\n}\n\nhistory = []\nbandit_state = {k:[1,1] for k in agents.keys()}\n\n\ndef multi_armed_bandit_agent (observation, configuration):\n    \n    step_size = 3 \n    decay_rate = 1.1\n    \n    global history, bandit_state\n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None, file = 'history.csv'):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        if file is not None:\n            pd.DataFrame(history).to_csv(file, index = False)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n    \n    if observation.step == 0:\n        pass\n    else:\n        history = update_competitor_step(history, observation.lastOpponentAction)\n        \n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) / decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) / decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size/2\n                bandit_state[name][1] += step_size/2\n            \n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        \n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        #proba = bandit_state[k][0]/(bandit_state[k][0]+bandit_state[k][1])        \n        \n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)\n")
    __stickytape_write_module('bumblepuppy.py', 'code = compile(\n    """\n#                         WoofWoofWoof\n#                     Woof            Woof\n#                Woof                      Woof\n#              Woof                          Woof\n#             Woof  Centrifugal Bumble-puppy  Woof\n#              Woof                          Woof\n#                Woof                      Woof\n#                     Woof            Woof\n#                         WoofWoofWoof\n\nimport random\n\nnumber_of_predictors = 60 #yes, this really has 60 predictors.\nnumber_of_metapredictors = 4 #actually, I lied! This has 240 predictors.\n\n\nif not input:\n\tlimits = [50,20,6]\n\tbeat={\'R\':\'P\',\'P\':\'S\',\'S\':\'R\'}\n\turmoves=""\n\tmymoves=""\n\tDNAmoves=""\n\toutputs=[random.choice("RPS")]*number_of_metapredictors\n\tpredictorscore1=[3]*number_of_predictors\n\tpredictorscore2=[3]*number_of_predictors\n\tpredictorscore3=[3]*number_of_predictors\n\tpredictorscore4=[3]*number_of_predictors\n\tnuclease={\'RP\':\'a\',\'PS\':\'b\',\'SR\':\'c\',\'PR\':\'d\',\'SP\':\'e\',\'RS\':\'f\',\'RR\':\'g\',\'PP\':\'h\',\'SS\':\'i\'}\n\tlength=0\n\tpredictors=[random.choice("RPS")]*number_of_predictors\n\tmetapredictors=[random.choice("RPS")]*number_of_metapredictors\n\tmetapredictorscore=[3]*number_of_metapredictors\nelse:\n\n\tfor i in range(number_of_predictors):\n\t\t#metapredictor 1\n\t\tpredictorscore1[i]*=0.8\n\t\tpredictorscore1[i]+=(input==predictors[i])*3\n\t\tpredictorscore1[i]-=(input==beat[beat[predictors[i]]])*3\n\t\t#metapredictor 2: beat metapredictor 1 (probably contains a bug)\n\t\tpredictorscore2[i]*=0.8\n\t\tpredictorscore2[i]+=(output==predictors[i])*3\n\t\tpredictorscore2[i]-=(output==beat[beat[predictors[i]]])*3\n\t\t#metapredictor 3\n\t\tpredictorscore3[i]+=(input==predictors[i])*3\n\t\tif input==beat[beat[predictors[i]]]:\n\t\t\tpredictorscore3[i]=0\n\t\t#metapredictor 4: beat metapredictor 3 (probably contains a bug)\n\t\tpredictorscore4[i]+=(output==predictors[i])*3\n\t\tif output==beat[beat[predictors[i]]]:\n\t\t\tpredictorscore4[i]=0\n\t\t\t\n\tfor i in range(number_of_metapredictors):\n\t\tmetapredictorscore[i]*=0.96\n\t\tmetapredictorscore[i]+=(input==metapredictors[i])*3\n\t\tmetapredictorscore[i]-=(input==beat[beat[metapredictors[i]]])*3\n\t\t\n\t\n\t#Predictors 1-18: History matching\n\turmoves+=input\t\t\n\tmymoves+=output\n\tDNAmoves+=nuclease[input+output]\n\tlength+=1\n\t\n\tfor z in range(3):\n\t\tlimit = min([length,limits[z]])\n\t\tj=limit\n\t\twhile j>=1 and not DNAmoves[length-j:length] in DNAmoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = DNAmoves.rfind(DNAmoves[length-j:length],0,length-1) \n\t\t\tpredictors[0+6*z] = urmoves[j+i] \n\t\t\tpredictors[1+6*z] = beat[mymoves[j+i]] \n\t\tj=limit\t\t\t\n\t\twhile j>=1 and not urmoves[length-j:length] in urmoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = urmoves.rfind(urmoves[length-j:length],0,length-1) \n\t\t\tpredictors[2+6*z] = urmoves[j+i] \n\t\t\tpredictors[3+6*z] = beat[mymoves[j+i]] \n\t\tj=limit\n\t\twhile j>=1 and not mymoves[length-j:length] in mymoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = mymoves.rfind(mymoves[length-j:length],0,length-1) \n\t\t\tpredictors[4+6*z] = urmoves[j+i] \n\t\t\tpredictors[5+6*z] = beat[mymoves[j+i]]\n\t#Predictor 19,20: RNA Polymerase\t\t\n\tL=len(mymoves)\n\ti=DNAmoves.rfind(DNAmoves[L-j:L-1],0,L-2)\n\twhile i==-1:\n\t\tj-=1\n\t\ti=DNAmoves.rfind(DNAmoves[L-j:L-1],0,L-2)\n\t\tif j<2:\n\t\t\tbreak\n\tif i==-1 or j+i>=L:\n\t\tpredictors[18]=predictors[19]=random.choice("RPS")\n\telse:\n\t\tpredictors[18]=beat[mymoves[j+i]]\n\t\tpredictors[19]=urmoves[j+i]\n\n\t#Predictors 21-60: rotations of Predictors 1:20\n\tfor i in range(20,60):\n\t\tpredictors[i]=beat[beat[predictors[i-20]]] #Trying to second guess me?\n\t\n\tmetapredictors[0]=predictors[predictorscore1.index(max(predictorscore1))]\n\tmetapredictors[1]=beat[predictors[predictorscore2.index(max(predictorscore2))]]\n\tmetapredictors[2]=predictors[predictorscore3.index(max(predictorscore3))]\n\tmetapredictors[3]=beat[predictors[predictorscore4.index(max(predictorscore4))]]\n\t\n\t#compare predictors\noutput = beat[metapredictors[metapredictorscore.index(max(metapredictorscore))]]\nif max(metapredictorscore)<0:\n\toutput = beat[random.choice(urmoves)]\n""", \'<string>\', \'exec\')\ngg = {}\n\n\ndef run(observation, configuration):\n    global gg\n    global code\n    inp = \'\'\n    try:\n        inp = \'RPS\'[observation.lastOpponentAction]\n    except:\n        pass\n    gg[\'input\'] = inp\n    exec(code, gg)\n    return {\'R\': 0, \'P\': 1, \'S\': 2}[gg[\'output\']]\n')
    __stickytape_write_module('dllu.py', 'code = compile(\n    """\n# see also www.dllu.net/rps\n# remember, rpsrunner.py is extremely useful for offline testing, \n# here\'s a screenshot: http://i.imgur.com/DcO9M.png\nimport random\nnumPre = 30\nnumMeta = 6\nif not input:\n    limit = 8\n    beat={\'R\':\'P\',\'P\':\'S\',\'S\':\'R\'}\n    moves=[\'\',\'\',\'\',\'\']\n    pScore=[[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre]\n    centrifuge={\'RP\':0,\'PS\':1,\'SR\':2,\'PR\':3,\'SP\':4,\'RS\':5,\'RR\':6,\'PP\':7,\'SS\':8}\n    centripete={\'R\':0,\'P\':1,\'S\':2}\n    soma = [0,0,0,0,0,0,0,0,0];\n    rps = [1,1,1];\n    a="RPS"\n    best = [0,0,0];\n    length=0\n    p=[random.choice("RPS")]*numPre\n    m=[random.choice("RPS")]*numMeta\n    mScore=[5,2,5,2,4,2]\nelse:\n    for i in range(numPre):\n        pp = p[i]\n        bpp = beat[pp]\n        bbpp = beat[bpp]\n        pScore[0][i]=0.9*pScore[0][i]+((input==pp)-(input==bbpp))*3\n        pScore[1][i]=0.9*pScore[1][i]+((output==pp)-(output==bbpp))*3\n        pScore[2][i]=0.87*pScore[2][i]+(input==pp)*3.3-(input==bpp)*1.2-(input==bbpp)*2.3\n        pScore[3][i]=0.87*pScore[3][i]+(output==pp)*3.3-(output==bpp)*1.2-(output==bbpp)*2.3\n        pScore[4][i]=(pScore[4][i]+(input==pp)*3)*(1-(input==bbpp))\n        pScore[5][i]=(pScore[5][i]+(output==pp)*3)*(1-(output==bbpp))\n    for i in range(numMeta):\n        mScore[i]=0.96*(mScore[i]+(input==m[i])-(input==beat[beat[m[i]]]))\n    soma[centrifuge[input+output]] +=1;\n    rps[centripete[input]] +=1;\n    moves[0]+=str(centrifuge[input+output])\n    moves[1]+=input\n    moves[2]+=output\n    length+=1\n    for y in range(3):\n        j=min([length,limit])\n        while j>=1 and not moves[y][length-j:length] in moves[y][0:length-1]:\n            j-=1\n        i = moves[y].rfind(moves[y][length-j:length],0,length-1)\n        p[0+2*y] = moves[1][j+i] \n        p[1+2*y] = beat[moves[2][j+i]]\n    j=min([length,limit])\n    while j>=2 and not moves[0][length-j:length-1] in moves[0][0:length-2]:\n        j-=1\n    i = moves[0].rfind(moves[0][length-j:length-1],0,length-2)\n    if j+i>=length:\n        p[6] = p[7] = random.choice("RPS")\n    else:\n        p[6] = moves[1][j+i] \n        p[7] = beat[moves[2][j+i]]\n        \n    best[0] = soma[centrifuge[output+\'R\']]*rps[0]/rps[centripete[output]]\n    best[1] = soma[centrifuge[output+\'P\']]*rps[1]/rps[centripete[output]]\n    best[2] = soma[centrifuge[output+\'S\']]*rps[2]/rps[centripete[output]]\n    p[8] = p[9] = a[best.index(max(best))]\n    \n    for i in range(10,numPre):\n        p[i]=beat[beat[p[i-10]]]\n        \n    for i in range(0,numMeta,2):\n        m[i]=       p[pScore[i  ].index(max(pScore[i  ]))]\n        m[i+1]=beat[p[pScore[i+1].index(max(pScore[i+1]))]]\noutput = beat[m[mScore.index(max(mScore))]]\nif max(mScore)<0.07 or random.randint(3,40)>length:\n    output=beat[random.choice("RPS")]\n""", \'<string>\', \'exec\')\ngg = {}\n\n\ndef run(observation, configuration):\n    global gg\n    global code\n    inp = \'\'\n    try:\n        inp = \'RPS\'[observation.lastOpponentAction]\n    except:\n        pass\n    gg[\'input\'] = inp\n    exec(code, gg)\n    return {\'R\': 0, \'P\': 1, \'S\': 2}[gg[\'output\']]\n')
    __stickytape_write_module('greenberg.py', "# greenberg roshambo bot, winner of 2nd annual roshambo programming competition\n# http://webdocs.cs.ualberta.ca/~darse/rsbpc.html\n\n# original source by Andrzej Nagorko\n# http://www.mathpuzzle.com/greenberg.c\n\n# Python translation by Travis Erdman\n# https://github.com/erdman/roshambo\n\n\ndef player(my_moves, opp_moves):\n    import random\n    from operator import itemgetter\n    rps_to_text = ('rock','paper','scissors')\n    rps_to_num  = {'rock':0, 'paper':1, 'scissors':2}\n    wins_with = (1,2,0)      #superior\n    best_without = (2,0,1)   #inferior\n\n    lengths = (10, 20, 30, 40, 49, 0)\n    p_random = random.choice([0,1,2])  #called 'guess' in iocaine\n\n    TRIALS = 1000\n    score_table =((0,-1,1),(1,0,-1),(-1,1,0))\n    T = len(opp_moves)  #so T is number of trials completed\n\n    def min_index(values):\n        return min(enumerate(values), key=itemgetter(1))[0]\n\n    def max_index(values):\n        return max(enumerate(values), key=itemgetter(1))[0]\n\n    def find_best_prediction(l):  # l = len\n        bs = -TRIALS\n        bp = 0\n        if player.p_random_score > bs:\n            bs = player.p_random_score\n            bp = p_random\n        for i in range(3):\n            for j in range(24):\n                for k in range(4):\n                    new_bs = player.p_full_score[T%50][j][k][i] - (player.p_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.p_full[j][k] + i) % 3\n                for k in range(2):\n                    new_bs = player.r_full_score[T%50][j][k][i] - (player.r_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.r_full[j][k] + i) % 3\n            for j in range(2):\n                for k in range(2):\n                    new_bs = player.p_freq_score[T%50][j][k][i] - (player.p_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.p_freq[j][k] + i) % 3\n                    new_bs = player.r_freq_score[T%50][j][k][i] - (player.r_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.r_freq[j][k] + i) % 3\n        return bp\n\n\n    if not my_moves:\n        player.opp_history = [0]  #pad to match up with 1-based move indexing in original\n        player.my_history = [0]\n        player.gear = [[0] for _ in range(24)]\n        # init()\n        player.p_random_score = 0\n        player.p_full_score = [[[[0 for i in range(3)] for k in range(4)] for j in range(24)] for l in range(50)]\n        player.r_full_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(24)] for l in range(50)]\n        player.p_freq_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(2)] for l in range(50)]\n        player.r_freq_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(2)] for l in range(50)]\n        player.s_len = [0] * 6\n\n        player.p_full = [[0,0,0,0] for _ in range(24)]\n        player.r_full = [[0,0] for _ in range(24)]\n    else:\n        player.my_history.append(rps_to_num[my_moves[-1]])\n        player.opp_history.append(rps_to_num[opp_moves[-1]])\n        # update_scores()\n        player.p_random_score += score_table[p_random][player.opp_history[-1]]\n        player.p_full_score[T%50] = [[[player.p_full_score[(T+49)%50][j][k][i] + score_table[(player.p_full[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(4)] for j in range(24)]\n        player.r_full_score[T%50] = [[[player.r_full_score[(T+49)%50][j][k][i] + score_table[(player.r_full[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(24)]\n        player.p_freq_score[T%50] = [[[player.p_freq_score[(T+49)%50][j][k][i] + score_table[(player.p_freq[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(2)]\n        player.r_freq_score[T%50] = [[[player.r_freq_score[(T+49)%50][j][k][i] + score_table[(player.r_freq[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(2)]\n        player.s_len = [s + score_table[p][player.opp_history[-1]] for s,p in zip(player.s_len,player.p_len)]\n\n\n    # update_history_hash()\n    if not my_moves:\n        player.my_history_hash = [[0],[0],[0],[0]]\n        player.opp_history_hash = [[0],[0],[0],[0]]\n    else:\n        player.my_history_hash[0].append(player.my_history[-1])\n        player.opp_history_hash[0].append(player.opp_history[-1])\n        for i in range(1,4):\n            player.my_history_hash[i].append(player.my_history_hash[i-1][-1] * 3 + player.my_history[-1])\n            player.opp_history_hash[i].append(player.opp_history_hash[i-1][-1] * 3 + player.opp_history[-1])\n\n\n    #make_predictions()\n\n    for i in range(24):\n        player.gear[i].append((3 + player.opp_history[-1] - player.p_full[i][2]) % 3)\n        if T > 1:\n            player.gear[i][T] += 3 * player.gear[i][T-1]\n        player.gear[i][T] %= 9 # clearly there are 9 different gears, but original code only allocated 3 gear_freq's\n                               # code apparently worked, but got lucky with undefined behavior\n                               # I fixed by allocating gear_freq with length = 9\n    if not my_moves:\n        player.freq = [[0,0,0],[0,0,0]]\n        value = [[0,0,0],[0,0,0]]\n    else:\n        player.freq[0][player.my_history[-1]] += 1\n        player.freq[1][player.opp_history[-1]] += 1\n        value = [[(1000 * (player.freq[i][2] - player.freq[i][1])) / float(T),\n                  (1000 * (player.freq[i][0] - player.freq[i][2])) / float(T),\n                  (1000 * (player.freq[i][1] - player.freq[i][0])) / float(T)] for i in range(2)]\n    player.p_freq = [[wins_with[max_index(player.freq[i])], wins_with[max_index(value[i])]] for i in range(2)]\n    player.r_freq = [[best_without[min_index(player.freq[i])], best_without[min_index(value[i])]] for i in range(2)]\n\n    f = [[[[0,0,0] for k in range(4)] for j in range(2)] for i in range(3)]\n    t = [[[0,0,0,0] for j in range(2)] for i in range(3)]\n\n    m_len = [[0 for _ in range(T)] for i in range(3)]\n\n    for i in range(T-1,0,-1):\n        m_len[0][i] = 4\n        for j in range(4):\n            if player.my_history_hash[j][i] != player.my_history_hash[j][T]:\n                m_len[0][i] = j\n                break\n        for j in range(4):\n            if player.opp_history_hash[j][i] != player.opp_history_hash[j][T]:\n                m_len[1][i] = j\n                break\n        for j in range(4):\n            if player.my_history_hash[j][i] != player.my_history_hash[j][T] or player.opp_history_hash[j][i] != player.opp_history_hash[j][T]:\n                m_len[2][i] = j\n                break\n\n    for i in range(T-1,0,-1):\n        for j in range(3):\n            for k in range(m_len[j][i]):\n                f[j][0][k][player.my_history[i+1]] += 1\n                f[j][1][k][player.opp_history[i+1]] += 1\n                t[j][0][k] += 1\n                t[j][1][k] += 1\n\n                if t[j][0][k] == 1:\n                    player.p_full[j*8 + 0*4 + k][0] = wins_with[player.my_history[i+1]]\n                if t[j][1][k] == 1:\n                    player.p_full[j*8 + 1*4 + k][0] = wins_with[player.opp_history[i+1]]\n                if t[j][0][k] == 3:\n                    player.p_full[j*8 + 0*4 + k][1] = wins_with[max_index(f[j][0][k])]\n                    player.r_full[j*8 + 0*4 + k][0] = best_without[min_index(f[j][0][k])]\n                if t[j][1][k] == 3:\n                    player.p_full[j*8 + 1*4 + k][1] = wins_with[max_index(f[j][1][k])]\n                    player.r_full[j*8 + 1*4 + k][0] = best_without[min_index(f[j][1][k])]\n\n    for j in range(3):\n        for k in range(4):\n            player.p_full[j*8 + 0*4 + k][2] = wins_with[max_index(f[j][0][k])]\n            player.r_full[j*8 + 0*4 + k][1] = best_without[min_index(f[j][0][k])]\n\n            player.p_full[j*8 + 1*4 + k][2] = wins_with[max_index(f[j][1][k])]\n            player.r_full[j*8 + 1*4 + k][1] = best_without[min_index(f[j][1][k])]\n\n    for j in range(24):\n        gear_freq = [0] * 9 # was [0,0,0] because original code incorrectly only allocated array length 3\n\n        for i in range(T-1,0,-1):\n            if player.gear[j][i] == player.gear[j][T]:\n                gear_freq[player.gear[j][i+1]] += 1\n\n        #original source allocated to 9 positions of gear_freq array, but only allocated first three\n        #also, only looked at first 3 to find the max_index\n        #unclear whether to seek max index over all 9 gear_freq's or just first 3 (as original code)\n        player.p_full[j][3] = (player.p_full[j][1] + max_index(gear_freq)) % 3\n\n    # end make_predictions()\n\n    player.p_len = [find_best_prediction(l) for l in lengths]\n\n    return rps_to_num[rps_to_text[player.p_len[max_index(player.s_len)]]]\n\nopponent_hist, my_hist = [], []\nact = None\n\ndef greenberg_agent(observation, configuration):\n    global opponent_hist, my_hist, act\n    \n    rps_to_text = ('rock','paper','scissors')\n    if observation.step > 0:\n        my_hist.append(rps_to_text[act])\n        opponent_hist.append(rps_to_text[observation.lastOpponentAction])\n        \n    act = player(my_hist, opponent_hist)\n    return act\n")
    __stickytape_write_module('iou.py', 'import random\n\nclass Strategy:\n  def __init__(self):\n    # 2 different self.lengths of history, 3 kinds of history, both, mine, yours\n    # 3 different self.limit self.length of reverse learning\n    # 6 kinds of strategy based on Iocaine Powder\n    self.num_predictor = 27\n\n\n    self.len_rfind = [20]\n    self.limit = [10,20,60]\n    self.beat = { "R":"P" , "P":"S", "S":"R"}\n    self.not_lose = { "R":"PPR" , "P":"SSP" , "S":"RRS" } #50-50 chance\n    self.my_his   =""\n    self.your_his =""\n    self.both_his =""\n    self.list_predictor = [""]*self.num_predictor\n    self.length = 0\n    self.temp1 = { "PP":"1" , "PR":"2" , "PS":"3",\n              "RP":"4" , "RR":"5", "RS":"6",\n              "SP":"7" , "SR":"8", "SS":"9"}\n    self.temp2 = { "1":"PP","2":"PR","3":"PS",\n                "4":"RP","5":"RR","6":"RS",\n                "7":"SP","8":"SR","9":"SS"} \n    self.who_win = { "PP": 0, "PR":1 , "PS":-1,\n                "RP": -1,"RR":0, "RS":1,\n                "SP": 1, "SR":-1, "SS":0}\n    self.score_predictor = [0]*self.num_predictor\n    self.output = random.choice("RPS")\n    self.predictors = [self.output]*self.num_predictor\n\n\n  def prepare_next_move(self, prev_input):\n    input = prev_input\n\n    #update self.predictors\n    #"""\n    if len(self.list_predictor[0])<5:\n        front =0\n    else:\n        front =1\n    for i in range (self.num_predictor):\n        if self.predictors[i]==input:\n            result ="1"\n        else:\n            result ="0"\n        self.list_predictor[i] = self.list_predictor[i][front:5]+result #only 5 rounds before\n    #history matching 1-6\n    self.my_his += self.output\n    self.your_his += input\n    self.both_his += self.temp1[input+self.output]\n    self.length +=1\n    for i in range(1):\n        len_size = min(self.length,self.len_rfind[i])\n        j=len_size\n        #self.both_his\n        while j>=1 and not self.both_his[self.length-j:self.length] in self.both_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.both_his.rfind(self.both_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[0+6*i] = self.your_his[j+k]\n            self.predictors[1+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[0+6*i] = random.choice("RPS")\n            self.predictors[1+6*i] = random.choice("RPS")\n        j=len_size\n        #self.your_his\n        while j>=1 and not self.your_his[self.length-j:self.length] in self.your_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.your_his.rfind(self.your_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[2+6*i] = self.your_his[j+k]\n            self.predictors[3+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[2+6*i] = random.choice("RPS")\n            self.predictors[3+6*i] = random.choice("RPS")\n        j=len_size\n        #self.my_his\n        while j>=1 and not self.my_his[self.length-j:self.length] in self.my_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.my_his.rfind(self.my_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[4+6*i] = self.your_his[j+k]\n            self.predictors[5+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[4+6*i] = random.choice("RPS")\n            self.predictors[5+6*i] = random.choice("RPS")\n\n    for i in range(3):\n        temp =""\n        search = self.temp1[(self.output+input)] #last round\n        for start in range(2, min(self.limit[i],self.length) ):\n            if search == self.both_his[self.length-start]:\n                temp+=self.both_his[self.length-start+1]\n        if(temp==""):\n            self.predictors[6+i] = random.choice("RPS")\n        else:\n            collectR = {"P":0,"R":0,"S":0} #take win/lose from opponent into account\n            for sdf in temp:\n                next_move = self.temp2[sdf]\n                if(self.who_win[next_move]==-1):\n                    collectR[self.temp2[sdf][1]]+=3\n                elif(self.who_win[next_move]==0):\n                    collectR[self.temp2[sdf][1]]+=1\n                elif(self.who_win[next_move]==1):\n                    collectR[self.beat[self.temp2[sdf][0]]]+=1\n            max1 = -1\n            p1 =""\n            for key in collectR:\n                if(collectR[key]>max1):\n                    max1 = collectR[key]\n                    p1 += key\n            self.predictors[6+i] = random.choice(p1)\n    \n    #rotate 9-27:\n    for i in range(9,27):\n        self.predictors[i] = self.beat[self.beat[self.predictors[i-9]]]\n        \n    #choose a predictor\n    len_his = len(self.list_predictor[0])\n    for i in range(self.num_predictor):\n        sum = 0\n        for j in range(len_his):\n            if self.list_predictor[i][j]=="1":\n                sum+=(j+1)*(j+1)\n            else:\n                sum-=(j+1)*(j+1)\n        self.score_predictor[i] = sum\n    max_score = max(self.score_predictor)\n    #min_score = min(self.score_predictor)\n    #c_temp = {"R":0,"P":0,"S":0}\n    #for i in range (self.num_predictor):\n        #if self.score_predictor[i]==max_score:\n        #    c_temp[self.predictors[i]] +=1\n        #if self.score_predictor[i]==min_score:\n        #    c_temp[self.predictors[i]] -=1\n    if max_score>0:\n        predict = self.predictors[self.score_predictor.index(max_score)]\n    else:\n        predict = random.choice(self.your_his)\n    self.output = random.choice(self.not_lose[predict])\n    return self.output\n\n\nglobal GLOBAL_STRATEGY\nGLOBAL_STRATEGY = Strategy()\n\n\ndef agent(observation, configuration):\n  global GLOBAL_STRATEGY\n\n  # Action mapping\n  to_char = ["R", "P", "S"]\n  from_char = {"R": 0, "P": 1, "S": 2}\n\n  if observation.step > 0:\n    GLOBAL_STRATEGY.prepare_next_move(to_char[observation.lastOpponentAction])\n  action = from_char[GLOBAL_STRATEGY.output]\n  return action\n')
    __stickytape_write_module('randomforest.py', 'import random\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n\nactions =  np.empty((0,0), dtype = int)\nobservations =  np.empty((0,0), dtype = int)\ntotal_reward = 0\n\ndef random_forest_random(observation, configuration):\n    global actions, observations, total_reward\n    \n    if observation.step == 0:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        return action\n    \n    if observation.step == 1:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        observations = np.append(observations , [observation.lastOpponentAction])\n        # Keep track of score\n        winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n        if winner == 1:\n            total_reward = total_reward + 1\n        elif winner == 2:\n            total_reward = total_reward - 1        \n        return action\n\n    # Get Observation to make the tables (actions & obervations) even.\n    observations = np.append(observations , [observation.lastOpponentAction])\n    \n    # Prepare Data for training\n    # :-1 as we dont have feedback yet.\n    X_train = np.vstack((actions[:-1], observations[:-1])).T\n    \n    # Create Y by rolling observations to bring future a step earlier \n    shifted_observations = np.roll(observations, -1)\n    \n    # trim rolled & last element from rolled observations\n    y_train = shifted_observations[:-1].T\n    \n    # Set the history period. Long chains here will need a lot of time\n    if len(X_train) > 25:\n        random_window_size = 10 + random.randint(0,10)\n        X_train = X_train[-random_window_size:]\n        y_train = y_train[-random_window_size:]\n   \n    # Train a classifier model\n    model = RandomForestClassifier(n_estimators=25)\n    model.fit(X_train, y_train)\n\n    # Predict\n    X_test = np.empty((0,0), dtype = int)\n    X_test = np.append(X_test, [int(actions[-1]), observation.lastOpponentAction])\n    prediction = model.predict(X_test.reshape(1, -1))\n\n    # Keep track of score\n    winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n    if winner == 1:\n        total_reward = total_reward + 1\n    elif winner == 2:\n        total_reward = total_reward - 1\n   \n    # Prepare action\n    action = int((prediction + 1) % 3)\n    \n    # If losing a bit then change strategy and break the patterns by playing a bit random\n    if total_reward < -2:\n        win_tie = random.randint(0,1)\n        action = int((prediction + win_tie) % 3)\n\n    # Update actions\n    actions = np.append(actions , [action])\n\n    # Action \n    return action')
    __stickytape_write_module('transition.py', 'import numpy as np\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))')
    __stickytape_write_module('iocaine.py', '\nimport random\n\n\ndef recall(age, hist):\n    """Looking at the last \'age\' points in \'hist\', finds the\n    last point with the longest similarity to the current point,\n    returning 0 if none found."""\n    end, length = 0, 0\n    for past in range(1, min(age + 1, len(hist) - 1)):\n        if length >= len(hist) - past: break\n        for i in range(-1 - length, 0):\n            if hist[i - past] != hist[i]: break\n        else:\n            for length in range(length + 1, len(hist) - past):\n                if hist[-past - length - 1] != hist[-length - 1]: break\n            else: length += 1\n            end = len(hist) - past\n    return end\n\ndef beat(i):\n    return (i + 1) % 3\ndef loseto(i):\n    return (i - 1) % 3\n\nclass Stats:\n    """Maintains three running counts and returns the highest count based\n         on any given time horizon and threshold."""\n    def __init__(self):\n        self.sum = [[0, 0, 0]]\n    def add(self, move, score):\n        self.sum[-1][move] += score\n    def advance(self):\n        self.sum.append(self.sum[-1])\n    def max(self, age, default, score):\n        if age >= len(self.sum): diff = self.sum[-1]\n        else: diff = [self.sum[-1][i] - self.sum[-1 - age][i] for i in range(3)]\n        m = max(diff)\n        if m > score: return diff.index(m), m\n        return default, score\n\nclass Predictor:\n    """The basic iocaine second- and triple-guesser.    Maintains stats on the\n         past benefits of trusting or second- or triple-guessing a given strategy,\n         and returns the prediction of that strategy (or the second- or triple-\n         guess) if past stats are deviating from zero farther than the supplied\n         "best" guess so far."""\n    def __init__(self):\n        self.stats = Stats()\n        self.lastguess = -1\n    def addguess(self, lastmove, guess):\n        if lastmove != -1:\n            diff = (lastmove - self.prediction) % 3\n            self.stats.add(beat(diff), 1)\n            self.stats.add(loseto(diff), -1)\n            self.stats.advance()\n        self.prediction = guess\n    def bestguess(self, age, best):\n        bestdiff = self.stats.max(age, (best[0] - self.prediction) % 3, best[1])\n        return (bestdiff[0] + self.prediction) % 3, bestdiff[1]\n\nages = [1000, 100, 10, 5, 2, 1]\n\nclass Iocaine:\n\n    def __init__(self):\n        """Build second-guessers for 50 strategies: 36 history-based strategies,\n             12 simple frequency-based strategies, the constant-move strategy, and\n             the basic random-number-generator strategy.    Also build 6 meta second\n             guessers to evaluate 6 different time horizons on which to score\n             the 50 strategies\' second-guesses."""\n        self.predictors = []\n        self.predict_history = self.predictor((len(ages), 2, 3))\n        self.predict_frequency = self.predictor((len(ages), 2))\n        self.predict_fixed = self.predictor()\n        self.predict_random = self.predictor()\n        self.predict_meta = [Predictor() for a in range(len(ages))]\n        self.stats = [Stats() for i in range(2)]\n        self.histories = [[], [], []]\n\n    def predictor(self, dims=None):\n        """Returns a nested array of predictor objects, of the given dimensions."""\n        if dims: return [self.predictor(dims[1:]) for i in range(dims[0])]\n        self.predictors.append(Predictor())\n        return self.predictors[-1]\n\n    def move(self, them):\n        """The main iocaine "move" function."""\n\n        # histories[0] stores our moves (last one already previously decided);\n        # histories[1] stores their moves (last one just now being supplied to us);\n        # histories[2] stores pairs of our and their last moves.\n        # stats[0] and stats[1] are running counters our recent moves and theirs.\n        if them != -1:\n            self.histories[1].append(them)\n            self.histories[2].append((self.histories[0][-1], them))\n            for watch in range(2):\n                self.stats[watch].add(self.histories[watch][-1], 1)\n\n        # Execute the basic RNG strategy and the fixed-move strategy.\n        rand = random.randrange(3)\n        self.predict_random.addguess(them, rand)\n        self.predict_fixed.addguess(them, 0)\n\n        # Execute the history and frequency stratgies.\n        for a, age in enumerate(ages):\n            # For each time window, there are three ways to recall a similar time:\n            # (0) by history of my moves; (1) their moves; or (2) pairs of moves.\n            # Set "best" to these three timeframes (zero if no matching time).\n            best = [recall(age, hist) for hist in self.histories]\n            for mimic in range(2):\n                # For each similar historical moment, there are two ways to anticipate\n                # the future: by mimicing what their move was; or mimicing what my\n                # move was.    If there were no similar moments, just move randomly.\n                for watch, when in enumerate(best):\n                    if not when: move = rand\n                    else: move = self.histories[mimic][when]\n                    self.predict_history[a][mimic][watch].addguess(them, move)\n                # Also we can anticipate the future by expecting it to be the same\n                # as the most frequent past (either counting their moves or my moves).\n                mostfreq, score = self.stats[mimic].max(age, rand, -1)\n                self.predict_frequency[a][mimic].addguess(them, mostfreq)\n\n        # All the predictors have been updated, but we have not yet scored them\n        # and chosen a winner for this round.    There are several timeframes\n        # on which we can score second-guessing, and we don\'t know timeframe will\n        # do best.    So score all 50 predictors on all 6 timeframes, and record\n        # the best 6 predictions in meta predictors, one for each timeframe.\n        for meta, age in enumerate(ages):\n            best = (-1, -1)\n            for predictor in self.predictors:\n                best = predictor.bestguess(age, best)\n            self.predict_meta[meta].addguess(them, best[0])\n\n        # Finally choose the best meta prediction from the final six, scoring\n        # these against each other on the whole-game timeframe. \n        best = (-1, -1)\n        for meta in range(len(ages)):\n            best = self.predict_meta[meta].bestguess(len(self.histories[0]) , best) \n\n        # We\'ve picked a next move.    Record our move in histories[0] for next time.\n        self.histories[0].append(best[0])\n\n        # And return it.\n        return best[0]\n\niocaine = None\n\ndef iocaine_agent(observation, configuration):\n    global iocaine\n    if observation.step == 0:\n        iocaine = Iocaine()\n        act = iocaine.move(-1)\n    else:\n        act = iocaine.move(observation.lastOpponentAction)\n        \n    return act\n')
    __stickytape_write_module('markov.py', "\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)\n")
    __stickytape_write_module('tree.py', "\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    features = np.append(features, rollouts['steps'])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    \n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action\n")
    __stickytape_write_module('beta_copycat.py', "\nimport pandas as pd\nimport numpy as np\nimport json\nfrom random import randrange\n    \n\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n\n        \nclass rps(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def rps(self, history):\n        return self.shift % 3\n    \n\nagents = {    \n    'rps_0': rps(0),\n    'rps_1': rps(1),\n    'rps_2': rps(2),\n}\n\nhistory = []\nbandit_state = {k:[1,1] for k in agents.keys()}\n\n\ndef multi_armed_bandit_agent (observation, configuration):\n    \n    step_size = 3 \n    decay_rate = 1.1\n    \n    global history, bandit_state\n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None, file = 'history1.csv'):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        if file is not None:\n            pd.DataFrame(history).to_csv(file, index = False)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n    \n    if observation.step == 0:\n        pass\n    else:\n        history = update_competitor_step(history, observation.lastOpponentAction)\n        \n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) / decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) / decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size/2\n                bandit_state[name][1] += step_size/2\n            \n    with open('bandit1.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        \n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        #proba = bandit_state[k][0]/(bandit_state[k][0]+bandit_state[k][1])        \n        \n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)\n")
    __stickytape_write_module('bumblepuppy_copycat.py', 'code = compile(\n    """\n#                         WoofWoofWoof\n#                     Woof            Woof\n#                Woof                      Woof\n#              Woof                          Woof\n#             Woof  Centrifugal Bumble-puppy  Woof\n#              Woof                          Woof\n#                Woof                      Woof\n#                     Woof            Woof\n#                         WoofWoofWoof\n\nimport random\n\nnumber_of_predictors = 60 #yes, this really has 60 predictors.\nnumber_of_metapredictors = 4 #actually, I lied! This has 240 predictors.\n\n\nif not input:\n\tlimits = [50,20,6]\n\tbeat={\'R\':\'P\',\'P\':\'S\',\'S\':\'R\'}\n\turmoves=""\n\tmymoves=""\n\tDNAmoves=""\n\toutputs=[random.choice("RPS")]*number_of_metapredictors\n\tpredictorscore1=[3]*number_of_predictors\n\tpredictorscore2=[3]*number_of_predictors\n\tpredictorscore3=[3]*number_of_predictors\n\tpredictorscore4=[3]*number_of_predictors\n\tnuclease={\'RP\':\'a\',\'PS\':\'b\',\'SR\':\'c\',\'PR\':\'d\',\'SP\':\'e\',\'RS\':\'f\',\'RR\':\'g\',\'PP\':\'h\',\'SS\':\'i\'}\n\tlength=0\n\tpredictors=[random.choice("RPS")]*number_of_predictors\n\tmetapredictors=[random.choice("RPS")]*number_of_metapredictors\n\tmetapredictorscore=[3]*number_of_metapredictors\nelse:\n\n\tfor i in range(number_of_predictors):\n\t\t#metapredictor 1\n\t\tpredictorscore1[i]*=0.8\n\t\tpredictorscore1[i]+=(input==predictors[i])*3\n\t\tpredictorscore1[i]-=(input==beat[beat[predictors[i]]])*3\n\t\t#metapredictor 2: beat metapredictor 1 (probably contains a bug)\n\t\tpredictorscore2[i]*=0.8\n\t\tpredictorscore2[i]+=(output==predictors[i])*3\n\t\tpredictorscore2[i]-=(output==beat[beat[predictors[i]]])*3\n\t\t#metapredictor 3\n\t\tpredictorscore3[i]+=(input==predictors[i])*3\n\t\tif input==beat[beat[predictors[i]]]:\n\t\t\tpredictorscore3[i]=0\n\t\t#metapredictor 4: beat metapredictor 3 (probably contains a bug)\n\t\tpredictorscore4[i]+=(output==predictors[i])*3\n\t\tif output==beat[beat[predictors[i]]]:\n\t\t\tpredictorscore4[i]=0\n\t\t\t\n\tfor i in range(number_of_metapredictors):\n\t\tmetapredictorscore[i]*=0.96\n\t\tmetapredictorscore[i]+=(input==metapredictors[i])*3\n\t\tmetapredictorscore[i]-=(input==beat[beat[metapredictors[i]]])*3\n\t\t\n\t\n\t#Predictors 1-18: History matching\n\turmoves+=input\t\t\n\tmymoves+=output\n\tDNAmoves+=nuclease[input+output]\n\tlength+=1\n\t\n\tfor z in range(3):\n\t\tlimit = min([length,limits[z]])\n\t\tj=limit\n\t\twhile j>=1 and not DNAmoves[length-j:length] in DNAmoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = DNAmoves.rfind(DNAmoves[length-j:length],0,length-1) \n\t\t\tpredictors[0+6*z] = urmoves[j+i] \n\t\t\tpredictors[1+6*z] = beat[mymoves[j+i]] \n\t\tj=limit\t\t\t\n\t\twhile j>=1 and not urmoves[length-j:length] in urmoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = urmoves.rfind(urmoves[length-j:length],0,length-1) \n\t\t\tpredictors[2+6*z] = urmoves[j+i] \n\t\t\tpredictors[3+6*z] = beat[mymoves[j+i]] \n\t\tj=limit\n\t\twhile j>=1 and not mymoves[length-j:length] in mymoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = mymoves.rfind(mymoves[length-j:length],0,length-1) \n\t\t\tpredictors[4+6*z] = urmoves[j+i] \n\t\t\tpredictors[5+6*z] = beat[mymoves[j+i]]\n\t#Predictor 19,20: RNA Polymerase\t\t\n\tL=len(mymoves)\n\ti=DNAmoves.rfind(DNAmoves[L-j:L-1],0,L-2)\n\twhile i==-1:\n\t\tj-=1\n\t\ti=DNAmoves.rfind(DNAmoves[L-j:L-1],0,L-2)\n\t\tif j<2:\n\t\t\tbreak\n\tif i==-1 or j+i>=L:\n\t\tpredictors[18]=predictors[19]=random.choice("RPS")\n\telse:\n\t\tpredictors[18]=beat[mymoves[j+i]]\n\t\tpredictors[19]=urmoves[j+i]\n\n\t#Predictors 21-60: rotations of Predictors 1:20\n\tfor i in range(20,60):\n\t\tpredictors[i]=beat[beat[predictors[i-20]]] #Trying to second guess me?\n\t\n\tmetapredictors[0]=predictors[predictorscore1.index(max(predictorscore1))]\n\tmetapredictors[1]=beat[predictors[predictorscore2.index(max(predictorscore2))]]\n\tmetapredictors[2]=predictors[predictorscore3.index(max(predictorscore3))]\n\tmetapredictors[3]=beat[predictors[predictorscore4.index(max(predictorscore4))]]\n\t\n\t#compare predictors\noutput = beat[metapredictors[metapredictorscore.index(max(metapredictorscore))]]\nif max(metapredictorscore)<0:\n\toutput = beat[random.choice(urmoves)]\n""", \'<string>\', \'exec\')\ngg = {}\n\n\ndef run(observation, configuration):\n    global gg\n    global code\n    inp = \'\'\n    try:\n        inp = \'RPS\'[observation.lastOpponentAction]\n    except:\n        pass\n    gg[\'input\'] = inp\n    exec(code, gg)\n    return {\'R\': 0, \'P\': 1, \'S\': 2}[gg[\'output\']]\n')
    __stickytape_write_module('dllu_copycat.py', 'code = compile(\n    """\n# see also www.dllu.net/rps\n# remember, rpsrunner.py is extremely useful for offline testing, \n# here\'s a screenshot: http://i.imgur.com/DcO9M.png\nimport random\nnumPre = 30\nnumMeta = 6\nif not input:\n    limit = 8\n    beat={\'R\':\'P\',\'P\':\'S\',\'S\':\'R\'}\n    moves=[\'\',\'\',\'\',\'\']\n    pScore=[[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre]\n    centrifuge={\'RP\':0,\'PS\':1,\'SR\':2,\'PR\':3,\'SP\':4,\'RS\':5,\'RR\':6,\'PP\':7,\'SS\':8}\n    centripete={\'R\':0,\'P\':1,\'S\':2}\n    soma = [0,0,0,0,0,0,0,0,0];\n    rps = [1,1,1];\n    a="RPS"\n    best = [0,0,0];\n    length=0\n    p=[random.choice("RPS")]*numPre\n    m=[random.choice("RPS")]*numMeta\n    mScore=[5,2,5,2,4,2]\nelse:\n    for i in range(numPre):\n        pp = p[i]\n        bpp = beat[pp]\n        bbpp = beat[bpp]\n        pScore[0][i]=0.9*pScore[0][i]+((input==pp)-(input==bbpp))*3\n        pScore[1][i]=0.9*pScore[1][i]+((output==pp)-(output==bbpp))*3\n        pScore[2][i]=0.87*pScore[2][i]+(input==pp)*3.3-(input==bpp)*1.2-(input==bbpp)*2.3\n        pScore[3][i]=0.87*pScore[3][i]+(output==pp)*3.3-(output==bpp)*1.2-(output==bbpp)*2.3\n        pScore[4][i]=(pScore[4][i]+(input==pp)*3)*(1-(input==bbpp))\n        pScore[5][i]=(pScore[5][i]+(output==pp)*3)*(1-(output==bbpp))\n    for i in range(numMeta):\n        mScore[i]=0.96*(mScore[i]+(input==m[i])-(input==beat[beat[m[i]]]))\n    soma[centrifuge[input+output]] +=1;\n    rps[centripete[input]] +=1;\n    moves[0]+=str(centrifuge[input+output])\n    moves[1]+=input\n    moves[2]+=output\n    length+=1\n    for y in range(3):\n        j=min([length,limit])\n        while j>=1 and not moves[y][length-j:length] in moves[y][0:length-1]:\n            j-=1\n        i = moves[y].rfind(moves[y][length-j:length],0,length-1)\n        p[0+2*y] = moves[1][j+i] \n        p[1+2*y] = beat[moves[2][j+i]]\n    j=min([length,limit])\n    while j>=2 and not moves[0][length-j:length-1] in moves[0][0:length-2]:\n        j-=1\n    i = moves[0].rfind(moves[0][length-j:length-1],0,length-2)\n    if j+i>=length:\n        p[6] = p[7] = random.choice("RPS")\n    else:\n        p[6] = moves[1][j+i] \n        p[7] = beat[moves[2][j+i]]\n        \n    best[0] = soma[centrifuge[output+\'R\']]*rps[0]/rps[centripete[output]]\n    best[1] = soma[centrifuge[output+\'P\']]*rps[1]/rps[centripete[output]]\n    best[2] = soma[centrifuge[output+\'S\']]*rps[2]/rps[centripete[output]]\n    p[8] = p[9] = a[best.index(max(best))]\n    \n    for i in range(10,numPre):\n        p[i]=beat[beat[p[i-10]]]\n        \n    for i in range(0,numMeta,2):\n        m[i]=       p[pScore[i  ].index(max(pScore[i  ]))]\n        m[i+1]=beat[p[pScore[i+1].index(max(pScore[i+1]))]]\noutput = beat[m[mScore.index(max(mScore))]]\nif max(mScore)<0.07 or random.randint(3,40)>length:\n    output=beat[random.choice("RPS")]\n""", \'<string>\', \'exec\')\ngg = {}\n\n\ndef run(observation, configuration):\n    global gg\n    global code\n    inp = \'\'\n    try:\n        inp = \'RPS\'[observation.lastOpponentAction]\n    except:\n        pass\n    gg[\'input\'] = inp\n    exec(code, gg)\n    return {\'R\': 0, \'P\': 1, \'S\': 2}[gg[\'output\']]\n')
    __stickytape_write_module('greenberg_copycat.py', "# greenberg roshambo bot, winner of 2nd annual roshambo programming competition\n# http://webdocs.cs.ualberta.ca/~darse/rsbpc.html\n\n# original source by Andrzej Nagorko\n# http://www.mathpuzzle.com/greenberg.c\n\n# Python translation by Travis Erdman\n# https://github.com/erdman/roshambo\n\n\ndef player(my_moves, opp_moves):\n    import random\n    from operator import itemgetter\n    rps_to_text = ('rock','paper','scissors')\n    rps_to_num  = {'rock':0, 'paper':1, 'scissors':2}\n    wins_with = (1,2,0)      #superior\n    best_without = (2,0,1)   #inferior\n\n    lengths = (10, 20, 30, 40, 49, 0)\n    p_random = random.choice([0,1,2])  #called 'guess' in iocaine\n\n    TRIALS = 1000\n    score_table =((0,-1,1),(1,0,-1),(-1,1,0))\n    T = len(opp_moves)  #so T is number of trials completed\n\n    def min_index(values):\n        return min(enumerate(values), key=itemgetter(1))[0]\n\n    def max_index(values):\n        return max(enumerate(values), key=itemgetter(1))[0]\n\n    def find_best_prediction(l):  # l = len\n        bs = -TRIALS\n        bp = 0\n        if player.p_random_score > bs:\n            bs = player.p_random_score\n            bp = p_random\n        for i in range(3):\n            for j in range(24):\n                for k in range(4):\n                    new_bs = player.p_full_score[T%50][j][k][i] - (player.p_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.p_full[j][k] + i) % 3\n                for k in range(2):\n                    new_bs = player.r_full_score[T%50][j][k][i] - (player.r_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.r_full[j][k] + i) % 3\n            for j in range(2):\n                for k in range(2):\n                    new_bs = player.p_freq_score[T%50][j][k][i] - (player.p_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.p_freq[j][k] + i) % 3\n                    new_bs = player.r_freq_score[T%50][j][k][i] - (player.r_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.r_freq[j][k] + i) % 3\n        return bp\n\n\n    if not my_moves:\n        player.opp_history = [0]  #pad to match up with 1-based move indexing in original\n        player.my_history = [0]\n        player.gear = [[0] for _ in range(24)]\n        # init()\n        player.p_random_score = 0\n        player.p_full_score = [[[[0 for i in range(3)] for k in range(4)] for j in range(24)] for l in range(50)]\n        player.r_full_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(24)] for l in range(50)]\n        player.p_freq_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(2)] for l in range(50)]\n        player.r_freq_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(2)] for l in range(50)]\n        player.s_len = [0] * 6\n\n        player.p_full = [[0,0,0,0] for _ in range(24)]\n        player.r_full = [[0,0] for _ in range(24)]\n    else:\n        player.my_history.append(rps_to_num[my_moves[-1]])\n        player.opp_history.append(rps_to_num[opp_moves[-1]])\n        # update_scores()\n        player.p_random_score += score_table[p_random][player.opp_history[-1]]\n        player.p_full_score[T%50] = [[[player.p_full_score[(T+49)%50][j][k][i] + score_table[(player.p_full[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(4)] for j in range(24)]\n        player.r_full_score[T%50] = [[[player.r_full_score[(T+49)%50][j][k][i] + score_table[(player.r_full[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(24)]\n        player.p_freq_score[T%50] = [[[player.p_freq_score[(T+49)%50][j][k][i] + score_table[(player.p_freq[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(2)]\n        player.r_freq_score[T%50] = [[[player.r_freq_score[(T+49)%50][j][k][i] + score_table[(player.r_freq[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(2)]\n        player.s_len = [s + score_table[p][player.opp_history[-1]] for s,p in zip(player.s_len,player.p_len)]\n\n\n    # update_history_hash()\n    if not my_moves:\n        player.my_history_hash = [[0],[0],[0],[0]]\n        player.opp_history_hash = [[0],[0],[0],[0]]\n    else:\n        player.my_history_hash[0].append(player.my_history[-1])\n        player.opp_history_hash[0].append(player.opp_history[-1])\n        for i in range(1,4):\n            player.my_history_hash[i].append(player.my_history_hash[i-1][-1] * 3 + player.my_history[-1])\n            player.opp_history_hash[i].append(player.opp_history_hash[i-1][-1] * 3 + player.opp_history[-1])\n\n\n    #make_predictions()\n\n    for i in range(24):\n        player.gear[i].append((3 + player.opp_history[-1] - player.p_full[i][2]) % 3)\n        if T > 1:\n            player.gear[i][T] += 3 * player.gear[i][T-1]\n        player.gear[i][T] %= 9 # clearly there are 9 different gears, but original code only allocated 3 gear_freq's\n                               # code apparently worked, but got lucky with undefined behavior\n                               # I fixed by allocating gear_freq with length = 9\n    if not my_moves:\n        player.freq = [[0,0,0],[0,0,0]]\n        value = [[0,0,0],[0,0,0]]\n    else:\n        player.freq[0][player.my_history[-1]] += 1\n        player.freq[1][player.opp_history[-1]] += 1\n        value = [[(1000 * (player.freq[i][2] - player.freq[i][1])) / float(T),\n                  (1000 * (player.freq[i][0] - player.freq[i][2])) / float(T),\n                  (1000 * (player.freq[i][1] - player.freq[i][0])) / float(T)] for i in range(2)]\n    player.p_freq = [[wins_with[max_index(player.freq[i])], wins_with[max_index(value[i])]] for i in range(2)]\n    player.r_freq = [[best_without[min_index(player.freq[i])], best_without[min_index(value[i])]] for i in range(2)]\n\n    f = [[[[0,0,0] for k in range(4)] for j in range(2)] for i in range(3)]\n    t = [[[0,0,0,0] for j in range(2)] for i in range(3)]\n\n    m_len = [[0 for _ in range(T)] for i in range(3)]\n\n    for i in range(T-1,0,-1):\n        m_len[0][i] = 4\n        for j in range(4):\n            if player.my_history_hash[j][i] != player.my_history_hash[j][T]:\n                m_len[0][i] = j\n                break\n        for j in range(4):\n            if player.opp_history_hash[j][i] != player.opp_history_hash[j][T]:\n                m_len[1][i] = j\n                break\n        for j in range(4):\n            if player.my_history_hash[j][i] != player.my_history_hash[j][T] or player.opp_history_hash[j][i] != player.opp_history_hash[j][T]:\n                m_len[2][i] = j\n                break\n\n    for i in range(T-1,0,-1):\n        for j in range(3):\n            for k in range(m_len[j][i]):\n                f[j][0][k][player.my_history[i+1]] += 1\n                f[j][1][k][player.opp_history[i+1]] += 1\n                t[j][0][k] += 1\n                t[j][1][k] += 1\n\n                if t[j][0][k] == 1:\n                    player.p_full[j*8 + 0*4 + k][0] = wins_with[player.my_history[i+1]]\n                if t[j][1][k] == 1:\n                    player.p_full[j*8 + 1*4 + k][0] = wins_with[player.opp_history[i+1]]\n                if t[j][0][k] == 3:\n                    player.p_full[j*8 + 0*4 + k][1] = wins_with[max_index(f[j][0][k])]\n                    player.r_full[j*8 + 0*4 + k][0] = best_without[min_index(f[j][0][k])]\n                if t[j][1][k] == 3:\n                    player.p_full[j*8 + 1*4 + k][1] = wins_with[max_index(f[j][1][k])]\n                    player.r_full[j*8 + 1*4 + k][0] = best_without[min_index(f[j][1][k])]\n\n    for j in range(3):\n        for k in range(4):\n            player.p_full[j*8 + 0*4 + k][2] = wins_with[max_index(f[j][0][k])]\n            player.r_full[j*8 + 0*4 + k][1] = best_without[min_index(f[j][0][k])]\n\n            player.p_full[j*8 + 1*4 + k][2] = wins_with[max_index(f[j][1][k])]\n            player.r_full[j*8 + 1*4 + k][1] = best_without[min_index(f[j][1][k])]\n\n    for j in range(24):\n        gear_freq = [0] * 9 # was [0,0,0] because original code incorrectly only allocated array length 3\n\n        for i in range(T-1,0,-1):\n            if player.gear[j][i] == player.gear[j][T]:\n                gear_freq[player.gear[j][i+1]] += 1\n\n        #original source allocated to 9 positions of gear_freq array, but only allocated first three\n        #also, only looked at first 3 to find the max_index\n        #unclear whether to seek max index over all 9 gear_freq's or just first 3 (as original code)\n        player.p_full[j][3] = (player.p_full[j][1] + max_index(gear_freq)) % 3\n\n    # end make_predictions()\n\n    player.p_len = [find_best_prediction(l) for l in lengths]\n\n    return rps_to_num[rps_to_text[player.p_len[max_index(player.s_len)]]]\n\nopponent_hist, my_hist = [], []\nact = None\n\ndef greenberg_agent(observation, configuration):\n    global opponent_hist, my_hist, act\n    \n    rps_to_text = ('rock','paper','scissors')\n    if observation.step > 0:\n        my_hist.append(rps_to_text[act])\n        opponent_hist.append(rps_to_text[observation.lastOpponentAction])\n        \n    act = player(my_hist, opponent_hist)\n    return act\n")
    __stickytape_write_module('iou_copycat.py', 'import random\n\nclass Strategy:\n  def __init__(self):\n    # 2 different self.lengths of history, 3 kinds of history, both, mine, yours\n    # 3 different self.limit self.length of reverse learning\n    # 6 kinds of strategy based on Iocaine Powder\n    self.num_predictor = 27\n\n\n    self.len_rfind = [20]\n    self.limit = [10,20,60]\n    self.beat = { "R":"P" , "P":"S", "S":"R"}\n    self.not_lose = { "R":"PPR" , "P":"SSP" , "S":"RRS" } #50-50 chance\n    self.my_his   =""\n    self.your_his =""\n    self.both_his =""\n    self.list_predictor = [""]*self.num_predictor\n    self.length = 0\n    self.temp1 = { "PP":"1" , "PR":"2" , "PS":"3",\n              "RP":"4" , "RR":"5", "RS":"6",\n              "SP":"7" , "SR":"8", "SS":"9"}\n    self.temp2 = { "1":"PP","2":"PR","3":"PS",\n                "4":"RP","5":"RR","6":"RS",\n                "7":"SP","8":"SR","9":"SS"} \n    self.who_win = { "PP": 0, "PR":1 , "PS":-1,\n                "RP": -1,"RR":0, "RS":1,\n                "SP": 1, "SR":-1, "SS":0}\n    self.score_predictor = [0]*self.num_predictor\n    self.output = random.choice("RPS")\n    self.predictors = [self.output]*self.num_predictor\n\n\n  def prepare_next_move(self, prev_input):\n    input = prev_input\n\n    #update self.predictors\n    #"""\n    if len(self.list_predictor[0])<5:\n        front =0\n    else:\n        front =1\n    for i in range (self.num_predictor):\n        if self.predictors[i]==input:\n            result ="1"\n        else:\n            result ="0"\n        self.list_predictor[i] = self.list_predictor[i][front:5]+result #only 5 rounds before\n    #history matching 1-6\n    self.my_his += self.output\n    self.your_his += input\n    self.both_his += self.temp1[input+self.output]\n    self.length +=1\n    for i in range(1):\n        len_size = min(self.length,self.len_rfind[i])\n        j=len_size\n        #self.both_his\n        while j>=1 and not self.both_his[self.length-j:self.length] in self.both_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.both_his.rfind(self.both_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[0+6*i] = self.your_his[j+k]\n            self.predictors[1+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[0+6*i] = random.choice("RPS")\n            self.predictors[1+6*i] = random.choice("RPS")\n        j=len_size\n        #self.your_his\n        while j>=1 and not self.your_his[self.length-j:self.length] in self.your_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.your_his.rfind(self.your_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[2+6*i] = self.your_his[j+k]\n            self.predictors[3+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[2+6*i] = random.choice("RPS")\n            self.predictors[3+6*i] = random.choice("RPS")\n        j=len_size\n        #self.my_his\n        while j>=1 and not self.my_his[self.length-j:self.length] in self.my_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.my_his.rfind(self.my_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[4+6*i] = self.your_his[j+k]\n            self.predictors[5+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[4+6*i] = random.choice("RPS")\n            self.predictors[5+6*i] = random.choice("RPS")\n\n    for i in range(3):\n        temp =""\n        search = self.temp1[(self.output+input)] #last round\n        for start in range(2, min(self.limit[i],self.length) ):\n            if search == self.both_his[self.length-start]:\n                temp+=self.both_his[self.length-start+1]\n        if(temp==""):\n            self.predictors[6+i] = random.choice("RPS")\n        else:\n            collectR = {"P":0,"R":0,"S":0} #take win/lose from opponent into account\n            for sdf in temp:\n                next_move = self.temp2[sdf]\n                if(self.who_win[next_move]==-1):\n                    collectR[self.temp2[sdf][1]]+=3\n                elif(self.who_win[next_move]==0):\n                    collectR[self.temp2[sdf][1]]+=1\n                elif(self.who_win[next_move]==1):\n                    collectR[self.beat[self.temp2[sdf][0]]]+=1\n            max1 = -1\n            p1 =""\n            for key in collectR:\n                if(collectR[key]>max1):\n                    max1 = collectR[key]\n                    p1 += key\n            self.predictors[6+i] = random.choice(p1)\n    \n    #rotate 9-27:\n    for i in range(9,27):\n        self.predictors[i] = self.beat[self.beat[self.predictors[i-9]]]\n        \n    #choose a predictor\n    len_his = len(self.list_predictor[0])\n    for i in range(self.num_predictor):\n        sum = 0\n        for j in range(len_his):\n            if self.list_predictor[i][j]=="1":\n                sum+=(j+1)*(j+1)\n            else:\n                sum-=(j+1)*(j+1)\n        self.score_predictor[i] = sum\n    max_score = max(self.score_predictor)\n    #min_score = min(self.score_predictor)\n    #c_temp = {"R":0,"P":0,"S":0}\n    #for i in range (self.num_predictor):\n        #if self.score_predictor[i]==max_score:\n        #    c_temp[self.predictors[i]] +=1\n        #if self.score_predictor[i]==min_score:\n        #    c_temp[self.predictors[i]] -=1\n    if max_score>0:\n        predict = self.predictors[self.score_predictor.index(max_score)]\n    else:\n        predict = random.choice(self.your_his)\n    self.output = random.choice(self.not_lose[predict])\n    return self.output\n\n\nglobal GLOBAL_STRATEGY\nGLOBAL_STRATEGY = Strategy()\n\n\ndef agent(observation, configuration):\n  global GLOBAL_STRATEGY\n\n  # Action mapping\n  to_char = ["R", "P", "S"]\n  from_char = {"R": 0, "P": 1, "S": 2}\n\n  if observation.step > 0:\n    GLOBAL_STRATEGY.prepare_next_move(to_char[observation.lastOpponentAction])\n  action = from_char[GLOBAL_STRATEGY.output]\n  return action\n')
    __stickytape_write_module('randomforest_copycat.py', 'import random\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n\nactions =  np.empty((0,0), dtype = int)\nobservations =  np.empty((0,0), dtype = int)\ntotal_reward = 0\n\ndef random_forest_random(observation, configuration):\n    global actions, observations, total_reward\n    \n    if observation.step == 0:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        return action\n    \n    if observation.step == 1:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        observations = np.append(observations , [observation.lastOpponentAction])\n        # Keep track of score\n        winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n        if winner == 1:\n            total_reward = total_reward + 1\n        elif winner == 2:\n            total_reward = total_reward - 1        \n        return action\n\n    # Get Observation to make the tables (actions & obervations) even.\n    observations = np.append(observations , [observation.lastOpponentAction])\n    \n    # Prepare Data for training\n    # :-1 as we dont have feedback yet.\n    X_train = np.vstack((actions[:-1], observations[:-1])).T\n    \n    # Create Y by rolling observations to bring future a step earlier \n    shifted_observations = np.roll(observations, -1)\n    \n    # trim rolled & last element from rolled observations\n    y_train = shifted_observations[:-1].T\n    \n    # Set the history period. Long chains here will need a lot of time\n    if len(X_train) > 25:\n        random_window_size = 10 + random.randint(0,10)\n        X_train = X_train[-random_window_size:]\n        y_train = y_train[-random_window_size:]\n   \n    # Train a classifier model\n    model = RandomForestClassifier(n_estimators=25)\n    model.fit(X_train, y_train)\n\n    # Predict\n    X_test = np.empty((0,0), dtype = int)\n    X_test = np.append(X_test, [int(actions[-1]), observation.lastOpponentAction])\n    prediction = model.predict(X_test.reshape(1, -1))\n\n    # Keep track of score\n    winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n    if winner == 1:\n        total_reward = total_reward + 1\n    elif winner == 2:\n        total_reward = total_reward - 1\n   \n    # Prepare action\n    action = int((prediction + 1) % 3)\n    \n    # If losing a bit then change strategy and break the patterns by playing a bit random\n    if total_reward < -2:\n        win_tie = random.randint(0,1)\n        action = int((prediction + win_tie) % 3)\n\n    # Update actions\n    actions = np.append(actions , [action])\n\n    # Action \n    return action')
    __stickytape_write_module('transition_copycat.py', 'import numpy as np\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))')
    __stickytape_write_module('iocaine_copycat.py', '\nimport random\n\n\ndef recall(age, hist):\n    """Looking at the last \'age\' points in \'hist\', finds the\n    last point with the longest similarity to the current point,\n    returning 0 if none found."""\n    end, length = 0, 0\n    for past in range(1, min(age + 1, len(hist) - 1)):\n        if length >= len(hist) - past: break\n        for i in range(-1 - length, 0):\n            if hist[i - past] != hist[i]: break\n        else:\n            for length in range(length + 1, len(hist) - past):\n                if hist[-past - length - 1] != hist[-length - 1]: break\n            else: length += 1\n            end = len(hist) - past\n    return end\n\ndef beat(i):\n    return (i + 1) % 3\ndef loseto(i):\n    return (i - 1) % 3\n\nclass Stats:\n    """Maintains three running counts and returns the highest count based\n         on any given time horizon and threshold."""\n    def __init__(self):\n        self.sum = [[0, 0, 0]]\n    def add(self, move, score):\n        self.sum[-1][move] += score\n    def advance(self):\n        self.sum.append(self.sum[-1])\n    def max(self, age, default, score):\n        if age >= len(self.sum): diff = self.sum[-1]\n        else: diff = [self.sum[-1][i] - self.sum[-1 - age][i] for i in range(3)]\n        m = max(diff)\n        if m > score: return diff.index(m), m\n        return default, score\n\nclass Predictor:\n    """The basic iocaine second- and triple-guesser.    Maintains stats on the\n         past benefits of trusting or second- or triple-guessing a given strategy,\n         and returns the prediction of that strategy (or the second- or triple-\n         guess) if past stats are deviating from zero farther than the supplied\n         "best" guess so far."""\n    def __init__(self):\n        self.stats = Stats()\n        self.lastguess = -1\n    def addguess(self, lastmove, guess):\n        if lastmove != -1:\n            diff = (lastmove - self.prediction) % 3\n            self.stats.add(beat(diff), 1)\n            self.stats.add(loseto(diff), -1)\n            self.stats.advance()\n        self.prediction = guess\n    def bestguess(self, age, best):\n        bestdiff = self.stats.max(age, (best[0] - self.prediction) % 3, best[1])\n        return (bestdiff[0] + self.prediction) % 3, bestdiff[1]\n\nages = [1000, 100, 10, 5, 2, 1]\n\nclass Iocaine:\n\n    def __init__(self):\n        """Build second-guessers for 50 strategies: 36 history-based strategies,\n             12 simple frequency-based strategies, the constant-move strategy, and\n             the basic random-number-generator strategy.    Also build 6 meta second\n             guessers to evaluate 6 different time horizons on which to score\n             the 50 strategies\' second-guesses."""\n        self.predictors = []\n        self.predict_history = self.predictor((len(ages), 2, 3))\n        self.predict_frequency = self.predictor((len(ages), 2))\n        self.predict_fixed = self.predictor()\n        self.predict_random = self.predictor()\n        self.predict_meta = [Predictor() for a in range(len(ages))]\n        self.stats = [Stats() for i in range(2)]\n        self.histories = [[], [], []]\n\n    def predictor(self, dims=None):\n        """Returns a nested array of predictor objects, of the given dimensions."""\n        if dims: return [self.predictor(dims[1:]) for i in range(dims[0])]\n        self.predictors.append(Predictor())\n        return self.predictors[-1]\n\n    def move(self, them):\n        """The main iocaine "move" function."""\n\n        # histories[0] stores our moves (last one already previously decided);\n        # histories[1] stores their moves (last one just now being supplied to us);\n        # histories[2] stores pairs of our and their last moves.\n        # stats[0] and stats[1] are running counters our recent moves and theirs.\n        if them != -1:\n            self.histories[1].append(them)\n            self.histories[2].append((self.histories[0][-1], them))\n            for watch in range(2):\n                self.stats[watch].add(self.histories[watch][-1], 1)\n\n        # Execute the basic RNG strategy and the fixed-move strategy.\n        rand = random.randrange(3)\n        self.predict_random.addguess(them, rand)\n        self.predict_fixed.addguess(them, 0)\n\n        # Execute the history and frequency stratgies.\n        for a, age in enumerate(ages):\n            # For each time window, there are three ways to recall a similar time:\n            # (0) by history of my moves; (1) their moves; or (2) pairs of moves.\n            # Set "best" to these three timeframes (zero if no matching time).\n            best = [recall(age, hist) for hist in self.histories]\n            for mimic in range(2):\n                # For each similar historical moment, there are two ways to anticipate\n                # the future: by mimicing what their move was; or mimicing what my\n                # move was.    If there were no similar moments, just move randomly.\n                for watch, when in enumerate(best):\n                    if not when: move = rand\n                    else: move = self.histories[mimic][when]\n                    self.predict_history[a][mimic][watch].addguess(them, move)\n                # Also we can anticipate the future by expecting it to be the same\n                # as the most frequent past (either counting their moves or my moves).\n                mostfreq, score = self.stats[mimic].max(age, rand, -1)\n                self.predict_frequency[a][mimic].addguess(them, mostfreq)\n\n        # All the predictors have been updated, but we have not yet scored them\n        # and chosen a winner for this round.    There are several timeframes\n        # on which we can score second-guessing, and we don\'t know timeframe will\n        # do best.    So score all 50 predictors on all 6 timeframes, and record\n        # the best 6 predictions in meta predictors, one for each timeframe.\n        for meta, age in enumerate(ages):\n            best = (-1, -1)\n            for predictor in self.predictors:\n                best = predictor.bestguess(age, best)\n            self.predict_meta[meta].addguess(them, best[0])\n\n        # Finally choose the best meta prediction from the final six, scoring\n        # these against each other on the whole-game timeframe. \n        best = (-1, -1)\n        for meta in range(len(ages)):\n            best = self.predict_meta[meta].bestguess(len(self.histories[0]) , best) \n\n        # We\'ve picked a next move.    Record our move in histories[0] for next time.\n        self.histories[0].append(best[0])\n\n        # And return it.\n        return best[0]\n\niocaine = None\n\ndef iocaine_agent(observation, configuration):\n    global iocaine\n    if observation.step == 0:\n        iocaine = Iocaine()\n        act = iocaine.move(-1)\n    else:\n        act = iocaine.move(observation.lastOpponentAction)\n        \n    return act\n')
    __stickytape_write_module('markov_copycat.py', "\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)\n")
    __stickytape_write_module('tree_copycat.py', "\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    features = np.append(features, rollouts['steps'])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    \n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action\n")
    import numpy as np
    # Import various methods.
    import beta
    import bumblepuppy
    import dllu
    import greenberg
    import iou
    import randomforest
    import transition
    import iocaine
    import markov
    import tree
    
    import beta_copycat
    import bumblepuppy_copycat
    import dllu_copycat
    import greenberg_copycat
    import iou_copycat
    import randomforest_copycat
    import transition_copycat
    import iocaine_copycat
    import markov_copycat
    import tree_copycat
    
    # Use Proportional Representation to find most favorable move.
    def proportionalRepresentation(array_of_moves, array_of_won_rounds, array_of_lost_rounds):
    
        best_prob = -1
        best_move = None
    
        for index, move in enumerate(array_of_moves):
            prob = np.random.beta(array_of_won_rounds[index], array_of_lost_rounds[index])
            if prob > best_prob:
                best_move = move.item()
                best_prob = prob
    
        return best_move
    
    
    def updateWonLostRounds(array_of_moves, opponent_move, array_won_rounds, array_lost_rounds):
    
        step_size = 3
        decay_rate = 1.1
    
        decay_won = (array_won_rounds - 1) / decay_rate + 1
        decay_lost = (array_lost_rounds - 1) / decay_rate + 1
    
        for index, move in enumerate(array_of_moves):
            if move == (opponent_move + 1) % 3:
                decay_won[index] += step_size
            elif move == (opponent_move + 2) % 3:
                decay_lost[index] += step_size
            else:
                decay_won[index] += step_size / 2
                decay_lost[index] += step_size / 2
        return decay_won, decay_lost
    
    main_step = 0
    main_method_steps = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
    main_won_rounds = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
                                1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
    main_lost_rounds = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
    
    def main(observation, configuration):
        global main_step
        global main_method_steps
        global main_won_rounds
        global main_lost_rounds
        # If not first step, update # of won round first.
        if main_step > 0:
            temp_win, temp_lost = updateWonLostRounds(main_method_steps, 
                                                          int(observation.lastOpponentAction), 
                                                          main_won_rounds, main_lost_rounds)
            main_won_rounds = temp_win
            main_lost_rounds = temp_lost
    
        # Compute moves generated by each method.
        step_random_forest = randomforest.random_forest_random(observation, configuration)
        step_transition = transition.transition_agent(observation, configuration)
        step_multi_armed = beta.multi_armed_bandit_agent (observation, configuration)
        step_dllu1 = dllu.run(observation, configuration)
        step_greenberg = greenberg.greenberg_agent(observation, configuration)
        step_IOU = iou.agent(observation, configuration)
        step_bumblepuppy = bumblepuppy.run(observation, configuration)
        step_iocaine = iocaine.iocaine_agent(observation, configuration)
        step_markov = markov.markov_agent(observation, configuration)
        step_tree = tree.agent(observation, configuration)

        if main_step > 0:
            observation.lastOpponentAction = step_random_forest
            step_copycat_randomforecast = randomforest_copycat.random_forest_random(observation, configuration)
            observation.lastOpponentAction = step_transition
            step_copycat_transition = transition_copycat.transition_agent(observation, configuration)
            observation.lastOpponentAction = step_multi_armed
            step_copycat_multi_armed = beta_copycat.multi_armed_bandit_agent (observation, configuration)
            observation.lastOpponentAction = step_dllu1
            step_copycat_dllu1 = dllu_copycat.run(observation, configuration)
            observation.lastOpponentAction = step_greenberg
            step_copycat_greenberg = greenberg_copycat.greenberg_agent(observation, configuration)
            observation.lastOpponentAction = step_IOU
            step_copycat_IOU = iou_copycat.agent(observation, configuration)
            observation.lastOpponentAction = step_bumblepuppy
            step_copycat_bumblepuppy = bumblepuppy_copycat.run(observation, configuration)
            observation.lastOpponentAction = step_iocaine
            step_copycat_iocaine = iocaine_copycat.iocaine_agent(observation, configuration)
            observation.lastOpponentAction = step_markov
            step_copycat_markov = markov_copycat.markov_agent(observation, configuration)
            observation.lastOpponentAction = step_tree
            step_copycat_tree = tree_copycat.agent(observation, configuration)
        else:
            step_copycat_randomforecast = randomforest_copycat.random_forest_random(observation, configuration)
            step_copycat_transition = transition_copycat.transition_agent(observation, configuration)
            step_copycat_multi_armed = beta_copycat.multi_armed_bandit_agent (observation, configuration)
            step_copycat_dllu1 = dllu_copycat.run(observation, configuration)
            step_copycat_greenberg = greenberg_copycat.greenberg_agent(observation, configuration)
            step_copycat_IOU = iou_copycat.agent(observation, configuration)
            step_copycat_bumblepuppy = bumblepuppy_copycat.run(observation, configuration)
            step_copycat_iocaine = iocaine_copycat.iocaine_agent(observation, configuration)
            step_copycat_markov = markov_copycat.markov_agent(observation, configuration)
            step_copycat_tree = tree_copycat.agent(observation, configuration)
            
        # Have all the moves in a np array.
        main_method_steps = np.array([step_random_forest, step_transition, step_multi_armed, step_dllu1, 
                                      step_greenberg, step_IOU, step_bumblepuppy, step_iocaine, 
                                      step_markov, step_tree, step_copycat_randomforecast, step_copycat_transition,
                                      step_copycat_multi_armed, step_copycat_dllu1, step_copycat_greenberg,
                                      step_copycat_IOU, step_copycat_bumblepuppy, step_copycat_iocaine,
                                      step_copycat_markov, step_copycat_tree])
    
        # If not first step, find current optimized step.
        if main_step > 0:
            selected_step = proportionalRepresentation(main_method_steps, main_won_rounds, 
                                                       main_lost_rounds)
            main_step += 1
            return selected_step
        # Otherwise, return random number.
        else:
            main_step += 1
            return np.random.randint(3)