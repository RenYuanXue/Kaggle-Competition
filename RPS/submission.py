import numpy as np
# Import various methods.
import beta
import bumblepuppy
import dllu
import greenberg
import iou
import memory
import randomforest
import transition
import iocaine
import markov
import tree
import rfind
import statpred
import stotransition

# Use Proportional Representation to find most favorable move.
def proportionalRepresentation(array_of_moves, array_of_won_rounds, array_of_lost_rounds, step):
    def filterWinrates(moves, win_rounds, lose_rounds):
        indexes = []
        rewards = win_rounds - lose_rounds
        for i in range(len(rewards)):
            # If one move's winrate is below 25%, remove it from the calculation.
            if rewards[i] < 0:
                indexes.append(i)
        moves = np.delete(moves, indexes)
        rewards = np.delete(rewards, indexes)
        return moves, rewards

    # Filter out the methods that has very low win rate.
    array_of_moves, array_of_rewards = filterWinrates(array_of_moves,
                                                      array_of_won_rounds,
                                                      array_of_lost_rounds)

    # If no method has threshold winrate, return a random move.
    if len(array_of_rewards) == 0:
        return np.random.randint(3)

    move_dict = {0: np.array([]), 1: np.array([]), 2:np.array([])}

    for index, element in enumerate(array_of_moves):
        move_dict[element] = np.append(move_dict[element], array_of_rewards[index])
    for move in move_dict:
        if len(move_dict[move]) == 0:
            move_dict[move] = 0
        else:
            move_dict[move] = sum(move_dict[move]) / len(move_dict[move])

    best_move = 0
    best_rating = -1
    for move in move_dict:
        if move_dict[move] > best_rating:
            best_move = move
            best_rating = move_dict[move]
    return best_move


def updateWonLostRounds(array_of_moves, opponent_move, array_won_rounds):
    current_win_round = np.array([0] * len(array_of_moves))
    current_lose_round = np.array([0] * len(array_of_moves))
    for index, move in enumerate(array_of_moves):
        if move == (opponent_move + 1) % 3:
            current_win_round[index] = 1
        if move == (opponent_move + 2) % 3:
            current_lose_round[index] = 1
    return current_win_round, current_lose_round


main_step = 0
main_won_rounds = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
main_lost_rounds = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
main_method_steps = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

def main(observation, configuration):
    global main_step
    global main_won_rounds
    global main_method_steps
    global main_lost_rounds
    # If not first step, update # of won round first.
    if main_step > 0:
        current_win_round, current_lose_round = updateWonLostRounds(main_method_steps, 
                                                                    int(observation.lastOpponentAction), 
                                                                    main_won_rounds)
        main_won_rounds = main_won_rounds + current_win_round
        main_lost_rounds = main_lost_rounds + current_lose_round

    # Compute moves generated by each method.
    step_random_forest = randomforest.random_forest_random(observation, configuration)
    step_transition = transition.transition_agent(observation, configuration)
    step_multi_armed = beta.multi_armed_bandit_agent (observation, configuration)
    step_dllu1 = dllu.run(observation, configuration)
    step_greenberg = greenberg.greenberg_agent(observation, configuration)
    step_IOU = iou.agent(observation, configuration)
    step_memory = memory.my_agent(observation, configuration)
    step_bumblepuppy = bumblepuppy.run(observation, configuration)
    step_iocaine = iocaine.iocaine_agent(observation, configuration)
    step_markov = markov.markov_agent(observation, configuration)
    step_rfind = rfind.run(observation, configuration)
    step_statpred = statpred.statistical_prediction_agent(observation, configuration)
    step_stotransition = stotransition.copy_opponent_agent(observation, configuration)
    step_tree = tree.agent(observation, configuration)

    # Have all the moves in a np array.
    method_steps = np.array([step_random_forest, step_transition, step_multi_armed, step_dllu1, 
                                  step_greenberg, step_IOU, step_memory, step_bumblepuppy, step_iocaine, 
                                  step_markov, step_rfind, step_statpred, step_stotransition, step_tree])

    counter_moves = (method_steps + 1) % 3

    main_method_steps = np.concatenate((method_steps, counter_moves))

    # If not first step, find current optimized step.
    if main_step > 0:
        selected_step = proportionalRepresentation(main_method_steps, main_won_rounds, 
                                                   main_lost_rounds, main_step)
        main_step += 1
        return selected_step
    # Otherwise, return random number.
    else:
        main_step += 1
        return np.random.randint(3)