#!/usr/bin/env python
import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    with open(os.path.join(partial_path, "__init__.py"), "w") as f:
                        f.write("\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "w") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('beta.py', "\nimport pandas as pd\nimport numpy as np\nimport json\nfrom random import randrange\n    \n\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n\n        \nclass rps(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def rps(self, history):\n        return self.shift % 3\n    \n\nagents = {    \n    'rps_0': rps(0),\n    'rps_1': rps(1),\n    'rps_2': rps(2),\n}\n\nhistory = []\nbandit_state = {k:[1,1] for k in agents.keys()}\n\n\ndef multi_armed_bandit_agent (observation, configuration):\n    \n    step_size = 3 \n    decay_rate = 1.1\n    \n    global history, bandit_state\n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None, file = 'history.csv'):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        if file is not None:\n            pd.DataFrame(history).to_csv(file, index = False)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n    \n    if observation.step == 0:\n        pass\n    else:\n        history = update_competitor_step(history, observation.lastOpponentAction)\n        \n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) / decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) / decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size/2\n                bandit_state[name][1] += step_size/2\n            \n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        \n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        #proba = bandit_state[k][0]/(bandit_state[k][0]+bandit_state[k][1])        \n        \n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)\n")
    __stickytape_write_module('bumblepuppy.py', 'code = compile(\n    """\n#                         WoofWoofWoof\n#                     Woof            Woof\n#                Woof                      Woof\n#              Woof                          Woof\n#             Woof  Centrifugal Bumble-puppy  Woof\n#              Woof                          Woof\n#                Woof                      Woof\n#                     Woof            Woof\n#                         WoofWoofWoof\n\nimport random\n\nnumber_of_predictors = 60 #yes, this really has 60 predictors.\nnumber_of_metapredictors = 4 #actually, I lied! This has 240 predictors.\n\n\nif not input:\n\tlimits = [50,20,6]\n\tbeat={\'R\':\'P\',\'P\':\'S\',\'S\':\'R\'}\n\turmoves=""\n\tmymoves=""\n\tDNAmoves=""\n\toutputs=[random.choice("RPS")]*number_of_metapredictors\n\tpredictorscore1=[3]*number_of_predictors\n\tpredictorscore2=[3]*number_of_predictors\n\tpredictorscore3=[3]*number_of_predictors\n\tpredictorscore4=[3]*number_of_predictors\n\tnuclease={\'RP\':\'a\',\'PS\':\'b\',\'SR\':\'c\',\'PR\':\'d\',\'SP\':\'e\',\'RS\':\'f\',\'RR\':\'g\',\'PP\':\'h\',\'SS\':\'i\'}\n\tlength=0\n\tpredictors=[random.choice("RPS")]*number_of_predictors\n\tmetapredictors=[random.choice("RPS")]*number_of_metapredictors\n\tmetapredictorscore=[3]*number_of_metapredictors\nelse:\n\n\tfor i in range(number_of_predictors):\n\t\t#metapredictor 1\n\t\tpredictorscore1[i]*=0.8\n\t\tpredictorscore1[i]+=(input==predictors[i])*3\n\t\tpredictorscore1[i]-=(input==beat[beat[predictors[i]]])*3\n\t\t#metapredictor 2: beat metapredictor 1 (probably contains a bug)\n\t\tpredictorscore2[i]*=0.8\n\t\tpredictorscore2[i]+=(output==predictors[i])*3\n\t\tpredictorscore2[i]-=(output==beat[beat[predictors[i]]])*3\n\t\t#metapredictor 3\n\t\tpredictorscore3[i]+=(input==predictors[i])*3\n\t\tif input==beat[beat[predictors[i]]]:\n\t\t\tpredictorscore3[i]=0\n\t\t#metapredictor 4: beat metapredictor 3 (probably contains a bug)\n\t\tpredictorscore4[i]+=(output==predictors[i])*3\n\t\tif output==beat[beat[predictors[i]]]:\n\t\t\tpredictorscore4[i]=0\n\t\t\t\n\tfor i in range(number_of_metapredictors):\n\t\tmetapredictorscore[i]*=0.96\n\t\tmetapredictorscore[i]+=(input==metapredictors[i])*3\n\t\tmetapredictorscore[i]-=(input==beat[beat[metapredictors[i]]])*3\n\t\t\n\t\n\t#Predictors 1-18: History matching\n\turmoves+=input\t\t\n\tmymoves+=output\n\tDNAmoves+=nuclease[input+output]\n\tlength+=1\n\t\n\tfor z in range(3):\n\t\tlimit = min([length,limits[z]])\n\t\tj=limit\n\t\twhile j>=1 and not DNAmoves[length-j:length] in DNAmoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = DNAmoves.rfind(DNAmoves[length-j:length],0,length-1) \n\t\t\tpredictors[0+6*z] = urmoves[j+i] \n\t\t\tpredictors[1+6*z] = beat[mymoves[j+i]] \n\t\tj=limit\t\t\t\n\t\twhile j>=1 and not urmoves[length-j:length] in urmoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = urmoves.rfind(urmoves[length-j:length],0,length-1) \n\t\t\tpredictors[2+6*z] = urmoves[j+i] \n\t\t\tpredictors[3+6*z] = beat[mymoves[j+i]] \n\t\tj=limit\n\t\twhile j>=1 and not mymoves[length-j:length] in mymoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = mymoves.rfind(mymoves[length-j:length],0,length-1) \n\t\t\tpredictors[4+6*z] = urmoves[j+i] \n\t\t\tpredictors[5+6*z] = beat[mymoves[j+i]]\n\t#Predictor 19,20: RNA Polymerase\t\t\n\tL=len(mymoves)\n\ti=DNAmoves.rfind(DNAmoves[L-j:L-1],0,L-2)\n\twhile i==-1:\n\t\tj-=1\n\t\ti=DNAmoves.rfind(DNAmoves[L-j:L-1],0,L-2)\n\t\tif j<2:\n\t\t\tbreak\n\tif i==-1 or j+i>=L:\n\t\tpredictors[18]=predictors[19]=random.choice("RPS")\n\telse:\n\t\tpredictors[18]=beat[mymoves[j+i]]\n\t\tpredictors[19]=urmoves[j+i]\n\n\t#Predictors 21-60: rotations of Predictors 1:20\n\tfor i in range(20,60):\n\t\tpredictors[i]=beat[beat[predictors[i-20]]] #Trying to second guess me?\n\t\n\tmetapredictors[0]=predictors[predictorscore1.index(max(predictorscore1))]\n\tmetapredictors[1]=beat[predictors[predictorscore2.index(max(predictorscore2))]]\n\tmetapredictors[2]=predictors[predictorscore3.index(max(predictorscore3))]\n\tmetapredictors[3]=beat[predictors[predictorscore4.index(max(predictorscore4))]]\n\t\n\t#compare predictors\noutput = beat[metapredictors[metapredictorscore.index(max(metapredictorscore))]]\nif max(metapredictorscore)<0:\n\toutput = beat[random.choice(urmoves)]\n""", \'<string>\', \'exec\')\ngg = {}\n\n\ndef run(observation, configuration):\n    global gg\n    global code\n    inp = \'\'\n    try:\n        inp = \'RPS\'[observation.lastOpponentAction]\n    except:\n        pass\n    gg[\'input\'] = inp\n    exec(code, gg)\n    return {\'R\': 0, \'P\': 1, \'S\': 2}[gg[\'output\']]\n')
    __stickytape_write_module('dllu.py', 'code = compile(\n    """\n# see also www.dllu.net/rps\n# remember, rpsrunner.py is extremely useful for offline testing, \n# here\'s a screenshot: http://i.imgur.com/DcO9M.png\nimport random\nnumPre = 30\nnumMeta = 6\nif not input:\n    limit = 8\n    beat={\'R\':\'P\',\'P\':\'S\',\'S\':\'R\'}\n    moves=[\'\',\'\',\'\',\'\']\n    pScore=[[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre]\n    centrifuge={\'RP\':0,\'PS\':1,\'SR\':2,\'PR\':3,\'SP\':4,\'RS\':5,\'RR\':6,\'PP\':7,\'SS\':8}\n    centripete={\'R\':0,\'P\':1,\'S\':2}\n    soma = [0,0,0,0,0,0,0,0,0];\n    rps = [1,1,1];\n    a="RPS"\n    best = [0,0,0];\n    length=0\n    p=[random.choice("RPS")]*numPre\n    m=[random.choice("RPS")]*numMeta\n    mScore=[5,2,5,2,4,2]\nelse:\n    for i in range(numPre):\n        pp = p[i]\n        bpp = beat[pp]\n        bbpp = beat[bpp]\n        pScore[0][i]=0.9*pScore[0][i]+((input==pp)-(input==bbpp))*3\n        pScore[1][i]=0.9*pScore[1][i]+((output==pp)-(output==bbpp))*3\n        pScore[2][i]=0.87*pScore[2][i]+(input==pp)*3.3-(input==bpp)*1.2-(input==bbpp)*2.3\n        pScore[3][i]=0.87*pScore[3][i]+(output==pp)*3.3-(output==bpp)*1.2-(output==bbpp)*2.3\n        pScore[4][i]=(pScore[4][i]+(input==pp)*3)*(1-(input==bbpp))\n        pScore[5][i]=(pScore[5][i]+(output==pp)*3)*(1-(output==bbpp))\n    for i in range(numMeta):\n        mScore[i]=0.96*(mScore[i]+(input==m[i])-(input==beat[beat[m[i]]]))\n    soma[centrifuge[input+output]] +=1;\n    rps[centripete[input]] +=1;\n    moves[0]+=str(centrifuge[input+output])\n    moves[1]+=input\n    moves[2]+=output\n    length+=1\n    for y in range(3):\n        j=min([length,limit])\n        while j>=1 and not moves[y][length-j:length] in moves[y][0:length-1]:\n            j-=1\n        i = moves[y].rfind(moves[y][length-j:length],0,length-1)\n        p[0+2*y] = moves[1][j+i] \n        p[1+2*y] = beat[moves[2][j+i]]\n    j=min([length,limit])\n    while j>=2 and not moves[0][length-j:length-1] in moves[0][0:length-2]:\n        j-=1\n    i = moves[0].rfind(moves[0][length-j:length-1],0,length-2)\n    if j+i>=length:\n        p[6] = p[7] = random.choice("RPS")\n    else:\n        p[6] = moves[1][j+i] \n        p[7] = beat[moves[2][j+i]]\n        \n    best[0] = soma[centrifuge[output+\'R\']]*rps[0]/rps[centripete[output]]\n    best[1] = soma[centrifuge[output+\'P\']]*rps[1]/rps[centripete[output]]\n    best[2] = soma[centrifuge[output+\'S\']]*rps[2]/rps[centripete[output]]\n    p[8] = p[9] = a[best.index(max(best))]\n    \n    for i in range(10,numPre):\n        p[i]=beat[beat[p[i-10]]]\n        \n    for i in range(0,numMeta,2):\n        m[i]=       p[pScore[i  ].index(max(pScore[i  ]))]\n        m[i+1]=beat[p[pScore[i+1].index(max(pScore[i+1]))]]\noutput = beat[m[mScore.index(max(mScore))]]\nif max(mScore)<0.07 or random.randint(3,40)>length:\n    output=beat[random.choice("RPS")]\n""", \'<string>\', \'exec\')\ngg = {}\n\n\ndef run(observation, configuration):\n    global gg\n    global code\n    inp = \'\'\n    try:\n        inp = \'RPS\'[observation.lastOpponentAction]\n    except:\n        pass\n    gg[\'input\'] = inp\n    exec(code, gg)\n    return {\'R\': 0, \'P\': 1, \'S\': 2}[gg[\'output\']]\n')
    __stickytape_write_module('greenberg.py', "# greenberg roshambo bot, winner of 2nd annual roshambo programming competition\n# http://webdocs.cs.ualberta.ca/~darse/rsbpc.html\n\n# original source by Andrzej Nagorko\n# http://www.mathpuzzle.com/greenberg.c\n\n# Python translation by Travis Erdman\n# https://github.com/erdman/roshambo\n\n\ndef player(my_moves, opp_moves):\n    import random\n    from operator import itemgetter\n    rps_to_text = ('rock','paper','scissors')\n    rps_to_num  = {'rock':0, 'paper':1, 'scissors':2}\n    wins_with = (1,2,0)      #superior\n    best_without = (2,0,1)   #inferior\n\n    lengths = (10, 20, 30, 40, 49, 0)\n    p_random = random.choice([0,1,2])  #called 'guess' in iocaine\n\n    TRIALS = 1000\n    score_table =((0,-1,1),(1,0,-1),(-1,1,0))\n    T = len(opp_moves)  #so T is number of trials completed\n\n    def min_index(values):\n        return min(enumerate(values), key=itemgetter(1))[0]\n\n    def max_index(values):\n        return max(enumerate(values), key=itemgetter(1))[0]\n\n    def find_best_prediction(l):  # l = len\n        bs = -TRIALS\n        bp = 0\n        if player.p_random_score > bs:\n            bs = player.p_random_score\n            bp = p_random\n        for i in range(3):\n            for j in range(24):\n                for k in range(4):\n                    new_bs = player.p_full_score[T%50][j][k][i] - (player.p_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.p_full[j][k] + i) % 3\n                for k in range(2):\n                    new_bs = player.r_full_score[T%50][j][k][i] - (player.r_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.r_full[j][k] + i) % 3\n            for j in range(2):\n                for k in range(2):\n                    new_bs = player.p_freq_score[T%50][j][k][i] - (player.p_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.p_freq[j][k] + i) % 3\n                    new_bs = player.r_freq_score[T%50][j][k][i] - (player.r_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.r_freq[j][k] + i) % 3\n        return bp\n\n\n    if not my_moves:\n        player.opp_history = [0]  #pad to match up with 1-based move indexing in original\n        player.my_history = [0]\n        player.gear = [[0] for _ in range(24)]\n        # init()\n        player.p_random_score = 0\n        player.p_full_score = [[[[0 for i in range(3)] for k in range(4)] for j in range(24)] for l in range(50)]\n        player.r_full_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(24)] for l in range(50)]\n        player.p_freq_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(2)] for l in range(50)]\n        player.r_freq_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(2)] for l in range(50)]\n        player.s_len = [0] * 6\n\n        player.p_full = [[0,0,0,0] for _ in range(24)]\n        player.r_full = [[0,0] for _ in range(24)]\n    else:\n        player.my_history.append(rps_to_num[my_moves[-1]])\n        player.opp_history.append(rps_to_num[opp_moves[-1]])\n        # update_scores()\n        player.p_random_score += score_table[p_random][player.opp_history[-1]]\n        player.p_full_score[T%50] = [[[player.p_full_score[(T+49)%50][j][k][i] + score_table[(player.p_full[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(4)] for j in range(24)]\n        player.r_full_score[T%50] = [[[player.r_full_score[(T+49)%50][j][k][i] + score_table[(player.r_full[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(24)]\n        player.p_freq_score[T%50] = [[[player.p_freq_score[(T+49)%50][j][k][i] + score_table[(player.p_freq[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(2)]\n        player.r_freq_score[T%50] = [[[player.r_freq_score[(T+49)%50][j][k][i] + score_table[(player.r_freq[j][k] + i) % 3][player.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(2)]\n        player.s_len = [s + score_table[p][player.opp_history[-1]] for s,p in zip(player.s_len,player.p_len)]\n\n\n    # update_history_hash()\n    if not my_moves:\n        player.my_history_hash = [[0],[0],[0],[0]]\n        player.opp_history_hash = [[0],[0],[0],[0]]\n    else:\n        player.my_history_hash[0].append(player.my_history[-1])\n        player.opp_history_hash[0].append(player.opp_history[-1])\n        for i in range(1,4):\n            player.my_history_hash[i].append(player.my_history_hash[i-1][-1] * 3 + player.my_history[-1])\n            player.opp_history_hash[i].append(player.opp_history_hash[i-1][-1] * 3 + player.opp_history[-1])\n\n\n    #make_predictions()\n\n    for i in range(24):\n        player.gear[i].append((3 + player.opp_history[-1] - player.p_full[i][2]) % 3)\n        if T > 1:\n            player.gear[i][T] += 3 * player.gear[i][T-1]\n        player.gear[i][T] %= 9 # clearly there are 9 different gears, but original code only allocated 3 gear_freq's\n                               # code apparently worked, but got lucky with undefined behavior\n                               # I fixed by allocating gear_freq with length = 9\n    if not my_moves:\n        player.freq = [[0,0,0],[0,0,0]]\n        value = [[0,0,0],[0,0,0]]\n    else:\n        player.freq[0][player.my_history[-1]] += 1\n        player.freq[1][player.opp_history[-1]] += 1\n        value = [[(1000 * (player.freq[i][2] - player.freq[i][1])) / float(T),\n                  (1000 * (player.freq[i][0] - player.freq[i][2])) / float(T),\n                  (1000 * (player.freq[i][1] - player.freq[i][0])) / float(T)] for i in range(2)]\n    player.p_freq = [[wins_with[max_index(player.freq[i])], wins_with[max_index(value[i])]] for i in range(2)]\n    player.r_freq = [[best_without[min_index(player.freq[i])], best_without[min_index(value[i])]] for i in range(2)]\n\n    f = [[[[0,0,0] for k in range(4)] for j in range(2)] for i in range(3)]\n    t = [[[0,0,0,0] for j in range(2)] for i in range(3)]\n\n    m_len = [[0 for _ in range(T)] for i in range(3)]\n\n    for i in range(T-1,0,-1):\n        m_len[0][i] = 4\n        for j in range(4):\n            if player.my_history_hash[j][i] != player.my_history_hash[j][T]:\n                m_len[0][i] = j\n                break\n        for j in range(4):\n            if player.opp_history_hash[j][i] != player.opp_history_hash[j][T]:\n                m_len[1][i] = j\n                break\n        for j in range(4):\n            if player.my_history_hash[j][i] != player.my_history_hash[j][T] or player.opp_history_hash[j][i] != player.opp_history_hash[j][T]:\n                m_len[2][i] = j\n                break\n\n    for i in range(T-1,0,-1):\n        for j in range(3):\n            for k in range(m_len[j][i]):\n                f[j][0][k][player.my_history[i+1]] += 1\n                f[j][1][k][player.opp_history[i+1]] += 1\n                t[j][0][k] += 1\n                t[j][1][k] += 1\n\n                if t[j][0][k] == 1:\n                    player.p_full[j*8 + 0*4 + k][0] = wins_with[player.my_history[i+1]]\n                if t[j][1][k] == 1:\n                    player.p_full[j*8 + 1*4 + k][0] = wins_with[player.opp_history[i+1]]\n                if t[j][0][k] == 3:\n                    player.p_full[j*8 + 0*4 + k][1] = wins_with[max_index(f[j][0][k])]\n                    player.r_full[j*8 + 0*4 + k][0] = best_without[min_index(f[j][0][k])]\n                if t[j][1][k] == 3:\n                    player.p_full[j*8 + 1*4 + k][1] = wins_with[max_index(f[j][1][k])]\n                    player.r_full[j*8 + 1*4 + k][0] = best_without[min_index(f[j][1][k])]\n\n    for j in range(3):\n        for k in range(4):\n            player.p_full[j*8 + 0*4 + k][2] = wins_with[max_index(f[j][0][k])]\n            player.r_full[j*8 + 0*4 + k][1] = best_without[min_index(f[j][0][k])]\n\n            player.p_full[j*8 + 1*4 + k][2] = wins_with[max_index(f[j][1][k])]\n            player.r_full[j*8 + 1*4 + k][1] = best_without[min_index(f[j][1][k])]\n\n    for j in range(24):\n        gear_freq = [0] * 9 # was [0,0,0] because original code incorrectly only allocated array length 3\n\n        for i in range(T-1,0,-1):\n            if player.gear[j][i] == player.gear[j][T]:\n                gear_freq[player.gear[j][i+1]] += 1\n\n        #original source allocated to 9 positions of gear_freq array, but only allocated first three\n        #also, only looked at first 3 to find the max_index\n        #unclear whether to seek max index over all 9 gear_freq's or just first 3 (as original code)\n        player.p_full[j][3] = (player.p_full[j][1] + max_index(gear_freq)) % 3\n\n    # end make_predictions()\n\n    player.p_len = [find_best_prediction(l) for l in lengths]\n\n    return rps_to_num[rps_to_text[player.p_len[max_index(player.s_len)]]]\n\nopponent_hist, my_hist = [], []\nact = None\n\ndef greenberg_agent(observation, configuration):\n    global opponent_hist, my_hist, act\n    \n    rps_to_text = ('rock','paper','scissors')\n    if observation.step > 0:\n        my_hist.append(rps_to_text[act])\n        opponent_hist.append(rps_to_text[observation.lastOpponentAction])\n        \n    act = player(my_hist, opponent_hist)\n    return act\n")
    __stickytape_write_module('iou.py', 'import random\n\nclass Strategy:\n  def __init__(self):\n    # 2 different self.lengths of history, 3 kinds of history, both, mine, yours\n    # 3 different self.limit self.length of reverse learning\n    # 6 kinds of strategy based on Iocaine Powder\n    self.num_predictor = 27\n\n\n    self.len_rfind = [20]\n    self.limit = [10,20,60]\n    self.beat = { "R":"P" , "P":"S", "S":"R"}\n    self.not_lose = { "R":"PPR" , "P":"SSP" , "S":"RRS" } #50-50 chance\n    self.my_his   =""\n    self.your_his =""\n    self.both_his =""\n    self.list_predictor = [""]*self.num_predictor\n    self.length = 0\n    self.temp1 = { "PP":"1" , "PR":"2" , "PS":"3",\n              "RP":"4" , "RR":"5", "RS":"6",\n              "SP":"7" , "SR":"8", "SS":"9"}\n    self.temp2 = { "1":"PP","2":"PR","3":"PS",\n                "4":"RP","5":"RR","6":"RS",\n                "7":"SP","8":"SR","9":"SS"} \n    self.who_win = { "PP": 0, "PR":1 , "PS":-1,\n                "RP": -1,"RR":0, "RS":1,\n                "SP": 1, "SR":-1, "SS":0}\n    self.score_predictor = [0]*self.num_predictor\n    self.output = random.choice("RPS")\n    self.predictors = [self.output]*self.num_predictor\n\n\n  def prepare_next_move(self, prev_input):\n    input = prev_input\n\n    #update self.predictors\n    #"""\n    if len(self.list_predictor[0])<5:\n        front =0\n    else:\n        front =1\n    for i in range (self.num_predictor):\n        if self.predictors[i]==input:\n            result ="1"\n        else:\n            result ="0"\n        self.list_predictor[i] = self.list_predictor[i][front:5]+result #only 5 rounds before\n    #history matching 1-6\n    self.my_his += self.output\n    self.your_his += input\n    self.both_his += self.temp1[input+self.output]\n    self.length +=1\n    for i in range(1):\n        len_size = min(self.length,self.len_rfind[i])\n        j=len_size\n        #self.both_his\n        while j>=1 and not self.both_his[self.length-j:self.length] in self.both_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.both_his.rfind(self.both_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[0+6*i] = self.your_his[j+k]\n            self.predictors[1+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[0+6*i] = random.choice("RPS")\n            self.predictors[1+6*i] = random.choice("RPS")\n        j=len_size\n        #self.your_his\n        while j>=1 and not self.your_his[self.length-j:self.length] in self.your_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.your_his.rfind(self.your_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[2+6*i] = self.your_his[j+k]\n            self.predictors[3+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[2+6*i] = random.choice("RPS")\n            self.predictors[3+6*i] = random.choice("RPS")\n        j=len_size\n        #self.my_his\n        while j>=1 and not self.my_his[self.length-j:self.length] in self.my_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.my_his.rfind(self.my_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[4+6*i] = self.your_his[j+k]\n            self.predictors[5+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[4+6*i] = random.choice("RPS")\n            self.predictors[5+6*i] = random.choice("RPS")\n\n    for i in range(3):\n        temp =""\n        search = self.temp1[(self.output+input)] #last round\n        for start in range(2, min(self.limit[i],self.length) ):\n            if search == self.both_his[self.length-start]:\n                temp+=self.both_his[self.length-start+1]\n        if(temp==""):\n            self.predictors[6+i] = random.choice("RPS")\n        else:\n            collectR = {"P":0,"R":0,"S":0} #take win/lose from opponent into account\n            for sdf in temp:\n                next_move = self.temp2[sdf]\n                if(self.who_win[next_move]==-1):\n                    collectR[self.temp2[sdf][1]]+=3\n                elif(self.who_win[next_move]==0):\n                    collectR[self.temp2[sdf][1]]+=1\n                elif(self.who_win[next_move]==1):\n                    collectR[self.beat[self.temp2[sdf][0]]]+=1\n            max1 = -1\n            p1 =""\n            for key in collectR:\n                if(collectR[key]>max1):\n                    max1 = collectR[key]\n                    p1 += key\n            self.predictors[6+i] = random.choice(p1)\n    \n    #rotate 9-27:\n    for i in range(9,27):\n        self.predictors[i] = self.beat[self.beat[self.predictors[i-9]]]\n        \n    #choose a predictor\n    len_his = len(self.list_predictor[0])\n    for i in range(self.num_predictor):\n        sum = 0\n        for j in range(len_his):\n            if self.list_predictor[i][j]=="1":\n                sum+=(j+1)*(j+1)\n            else:\n                sum-=(j+1)*(j+1)\n        self.score_predictor[i] = sum\n    max_score = max(self.score_predictor)\n    #min_score = min(self.score_predictor)\n    #c_temp = {"R":0,"P":0,"S":0}\n    #for i in range (self.num_predictor):\n        #if self.score_predictor[i]==max_score:\n        #    c_temp[self.predictors[i]] +=1\n        #if self.score_predictor[i]==min_score:\n        #    c_temp[self.predictors[i]] -=1\n    if max_score>0:\n        predict = self.predictors[self.score_predictor.index(max_score)]\n    else:\n        predict = random.choice(self.your_his)\n    self.output = random.choice(self.not_lose[predict])\n    return self.output\n\n\nglobal GLOBAL_STRATEGY\nGLOBAL_STRATEGY = Strategy()\n\n\ndef agent(observation, configuration):\n  global GLOBAL_STRATEGY\n\n  # Action mapping\n  to_char = ["R", "P", "S"]\n  from_char = {"R": 0, "P": 1, "S": 2}\n\n  if observation.step > 0:\n    GLOBAL_STRATEGY.prepare_next_move(to_char[observation.lastOpponentAction])\n  action = from_char[GLOBAL_STRATEGY.output]\n  return action\n')
    __stickytape_write_module('memory.py', '# start executing cells from here to rewrite submission.py\n\nimport random\n\ndef evaluate_pattern_efficiency(previous_step_result):\n    """ \n        evaluate efficiency of the pattern and, if pattern is inefficient,\n        remove it from agent\'s memory\n    """\n    pattern_group_index = previous_action["pattern_group_index"]\n    pattern_index = previous_action["pattern_index"]\n    pattern = groups_of_memory_patterns[pattern_group_index]["memory_patterns"][pattern_index]\n    pattern["reward"] += previous_step_result\n    # if pattern is inefficient\n    if pattern["reward"] <= EFFICIENCY_THRESHOLD:\n        # remove pattern from agent\'s memory\n        del groups_of_memory_patterns[pattern_group_index]["memory_patterns"][pattern_index]\n    \ndef find_action(group, group_index):\n    """ if possible, find my_action in this group of memory patterns """\n    if len(current_memory) > group["memory_length"]:\n        this_step_memory = current_memory[-group["memory_length"]:]\n        memory_pattern, pattern_index = find_pattern(group["memory_patterns"], this_step_memory, group["memory_length"])\n        if memory_pattern != None:\n            my_action_amount = 0\n            for action in memory_pattern["opp_next_actions"]:\n                # if this opponent\'s action occurred more times than currently chosen action\n                # or, if it occured the same amount of times and this one is choosen randomly among them\n                if (action["amount"] > my_action_amount or\n                        (action["amount"] == my_action_amount and random.random() > 0.5)):\n                    my_action_amount = action["amount"]\n                    my_action = action["response"]\n            return my_action, pattern_index\n    return None, None\n\ndef find_pattern(memory_patterns, memory, memory_length):\n    """ find appropriate pattern and its index in memory """\n    for i in range(len(memory_patterns)):\n        actions_matched = 0\n        for j in range(memory_length):\n            if memory_patterns[i]["actions"][j] == memory[j]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return memory_patterns[i], i\n    # appropriate pattern not found\n    return None, None\n\ndef get_step_result_for_my_agent(my_agent_action, opp_action):\n    """ \n        get result of the step for my_agent\n        1, 0 and -1 representing win, tie and lost results of the game respectively\n        reward will be taken from observation in the next release of kaggle environments\n    """\n    if my_agent_action == opp_action:\n        return 0\n    elif (my_agent_action == (opp_action + 1)) or (my_agent_action == 0 and opp_action == 2):\n        return 1\n    else:\n        return -1\n    \ndef update_current_memory(obs, my_action):\n    """ add my_agent\'s current step to current_memory """\n    # if there\'s too many actions in the current_memory\n    if len(current_memory) > current_memory_max_length:\n        # delete first two elements in current memory\n        # (actions of the oldest step in current memory)\n        del current_memory[:2]\n    # add agent\'s last action to agent\'s current memory\n    current_memory.append(my_action)\n    \ndef update_memory_pattern(obs, group):\n    """ if possible, update or add some memory pattern in this group """\n    # if length of current memory is suitable for this group of memory patterns\n    if len(current_memory) > group["memory_length"]:\n        # get memory of the previous step\n        # considering that last step actions of both agents are already present in current_memory\n        previous_step_memory = current_memory[-group["memory_length"] - 2 : -2]\n        previous_pattern, pattern_index = find_pattern(group["memory_patterns"], previous_step_memory, group["memory_length"])\n        if previous_pattern == None:\n            previous_pattern = {\n                # list of actions of both players\n                "actions": previous_step_memory.copy(),\n                # total reward earned by using this pattern\n                "reward": 0,\n                # list of observed opponent\'s actions after each occurrence of this pattern\n                "opp_next_actions": [\n                    # action that was made by opponent,\n                    # amount of times that action occurred,\n                    # what should be the response of my_agent\n                    {"action": 0, "amount": 0, "response": 1},\n                    {"action": 1, "amount": 0, "response": 2},\n                    {"action": 2, "amount": 0, "response": 0}\n                ]\n            }\n            group["memory_patterns"].append(previous_pattern)\n        # update previous_pattern\n        for action in previous_pattern["opp_next_actions"]:\n            if action["action"] == obs["lastOpponentAction"]:\n                action["amount"] += 1\n    \n# "%%writefile -a submission.py" will append the code below to submission.py,\n# it WILL NOT rewrite submission.py\n\n# maximum steps in a memory pattern\nSTEPS_MAX = 5\n# minimum steps in a memory pattern\nSTEPS_MIN = 3\n# lowest efficiency threshold of a memory pattern before being removed from agent\'s memory\nEFFICIENCY_THRESHOLD = -3\n# amount of steps between forced random actions\nFORCED_RANDOM_ACTION_INTERVAL = random.randint(STEPS_MIN, STEPS_MAX)\n\n# current memory of the agent\ncurrent_memory = []\n# previous action of my_agent\nprevious_action = {\n    "action": None,\n    # action was taken from pattern\n    "action_from_pattern": False,\n    "pattern_group_index": None,\n    "pattern_index": None\n}\n# amount of steps remained until next forced random action\nsteps_to_random = FORCED_RANDOM_ACTION_INTERVAL\n# maximum length of current_memory\ncurrent_memory_max_length = STEPS_MAX * 2\n# current reward of my_agent\n# will be taken from observation in the next release of kaggle environments\nreward = 0\n# memory length of patterns in first group\n# STEPS_MAX is multiplied by 2 to consider both my_agent\'s and opponent\'s actions\ngroup_memory_length = current_memory_max_length\n# list of groups of memory patterns\ngroups_of_memory_patterns = []\nfor i in range(STEPS_MAX, STEPS_MIN - 1, -1):\n    groups_of_memory_patterns.append({\n        # how many steps in a row are in the pattern\n        "memory_length": group_memory_length,\n        # list of memory patterns\n        "memory_patterns": []\n    })\n    group_memory_length -= 2\n    \n# "%%writefile -a submission.py" will append the code below to submission.py,\n# it WILL NOT rewrite submission.py\n\ndef my_agent(obs, conf):\n    """ your ad here """\n    # action of my_agent\n    my_action = None\n    \n    # forced random action\n    global steps_to_random\n    steps_to_random -= 1\n    if steps_to_random <= 0:\n        steps_to_random = FORCED_RANDOM_ACTION_INTERVAL\n        # choose action randomly\n        my_action = random.randint(0, 2)\n        # save action\'s data\n        previous_action["action"] = my_action\n        previous_action["action_from_pattern"] = False\n        previous_action["pattern_group_index"] = None\n        previous_action["pattern_index"] = None\n    \n    # if it\'s not first step\n    if obs["step"] > 0:\n        # add opponent\'s last step to current_memory\n        current_memory.append(obs["lastOpponentAction"])\n        # previous step won or lost\n        previous_step_result = get_step_result_for_my_agent(current_memory[-2], current_memory[-1])\n        global reward\n        reward += previous_step_result\n        # if previous action of my_agent was taken from pattern\n        if previous_action["action_from_pattern"]:\n            evaluate_pattern_efficiency(previous_step_result)\n    \n    for i in range(len(groups_of_memory_patterns)):\n        # if possible, update or add some memory pattern in this group\n        update_memory_pattern(obs, groups_of_memory_patterns[i])\n        # if action was not yet found\n        if my_action == None:\n            my_action, pattern_index = find_action(groups_of_memory_patterns[i], i)\n            if my_action != None:\n                # save action\'s data\n                previous_action["action"] = my_action\n                previous_action["action_from_pattern"] = True\n                previous_action["pattern_group_index"] = i\n                previous_action["pattern_index"] = pattern_index\n    \n    # if no action was found\n    if my_action == None:\n        # choose action randomly\n        my_action = random.randint(0, 2)\n        # save action\'s data\n        previous_action["action"] = my_action\n        previous_action["action_from_pattern"] = False\n        previous_action["pattern_group_index"] = None\n        previous_action["pattern_index"] = None\n    \n    # add my_agent\'s current step to current_memory\n    update_current_memory(obs, my_action)\n    return my_action\n')
    __stickytape_write_module('randomforest.py', 'import random\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n\nactions =  np.empty((0,0), dtype = int)\nobservations =  np.empty((0,0), dtype = int)\ntotal_reward = 0\n\ndef random_forest_random(observation, configuration):\n    global actions, observations, total_reward\n    \n    if observation.step == 0:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        return action\n    \n    if observation.step == 1:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        observations = np.append(observations , [observation.lastOpponentAction])\n        # Keep track of score\n        winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n        if winner == 1:\n            total_reward = total_reward + 1\n        elif winner == 2:\n            total_reward = total_reward - 1        \n        return action\n\n    # Get Observation to make the tables (actions & obervations) even.\n    observations = np.append(observations , [observation.lastOpponentAction])\n    \n    # Prepare Data for training\n    # :-1 as we dont have feedback yet.\n    X_train = np.vstack((actions[:-1], observations[:-1])).T\n    \n    # Create Y by rolling observations to bring future a step earlier \n    shifted_observations = np.roll(observations, -1)\n    \n    # trim rolled & last element from rolled observations\n    y_train = shifted_observations[:-1].T\n    \n    # Set the history period. Long chains here will need a lot of time\n    if len(X_train) > 25:\n        random_window_size = 10 + random.randint(0,10)\n        X_train = X_train[-random_window_size:]\n        y_train = y_train[-random_window_size:]\n   \n    # Train a classifier model\n    model = RandomForestClassifier(n_estimators=25)\n    model.fit(X_train, y_train)\n\n    # Predict\n    X_test = np.empty((0,0), dtype = int)\n    X_test = np.append(X_test, [int(actions[-1]), observation.lastOpponentAction])\n    prediction = model.predict(X_test.reshape(1, -1))\n\n    # Keep track of score\n    winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n    if winner == 1:\n        total_reward = total_reward + 1\n    elif winner == 2:\n        total_reward = total_reward - 1\n   \n    # Prepare action\n    action = int((prediction + 1) % 3)\n    \n    # If losing a bit then change strategy and break the patterns by playing a bit random\n    if total_reward < -2:\n        win_tie = random.randint(0,1)\n        action = int((prediction + win_tie) % 3)\n\n    # Update actions\n    actions = np.append(actions , [action])\n\n    # Action \n    return action')
    __stickytape_write_module('transition.py', 'import numpy as np\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))')
    __stickytape_write_module('iocaine.py', '\nimport random\n\n\ndef recall(age, hist):\n    """Looking at the last \'age\' points in \'hist\', finds the\n    last point with the longest similarity to the current point,\n    returning 0 if none found."""\n    end, length = 0, 0\n    for past in range(1, min(age + 1, len(hist) - 1)):\n        if length >= len(hist) - past: break\n        for i in range(-1 - length, 0):\n            if hist[i - past] != hist[i]: break\n        else:\n            for length in range(length + 1, len(hist) - past):\n                if hist[-past - length - 1] != hist[-length - 1]: break\n            else: length += 1\n            end = len(hist) - past\n    return end\n\ndef beat(i):\n    return (i + 1) % 3\ndef loseto(i):\n    return (i - 1) % 3\n\nclass Stats:\n    """Maintains three running counts and returns the highest count based\n         on any given time horizon and threshold."""\n    def __init__(self):\n        self.sum = [[0, 0, 0]]\n    def add(self, move, score):\n        self.sum[-1][move] += score\n    def advance(self):\n        self.sum.append(self.sum[-1])\n    def max(self, age, default, score):\n        if age >= len(self.sum): diff = self.sum[-1]\n        else: diff = [self.sum[-1][i] - self.sum[-1 - age][i] for i in range(3)]\n        m = max(diff)\n        if m > score: return diff.index(m), m\n        return default, score\n\nclass Predictor:\n    """The basic iocaine second- and triple-guesser.    Maintains stats on the\n         past benefits of trusting or second- or triple-guessing a given strategy,\n         and returns the prediction of that strategy (or the second- or triple-\n         guess) if past stats are deviating from zero farther than the supplied\n         "best" guess so far."""\n    def __init__(self):\n        self.stats = Stats()\n        self.lastguess = -1\n    def addguess(self, lastmove, guess):\n        if lastmove != -1:\n            diff = (lastmove - self.prediction) % 3\n            self.stats.add(beat(diff), 1)\n            self.stats.add(loseto(diff), -1)\n            self.stats.advance()\n        self.prediction = guess\n    def bestguess(self, age, best):\n        bestdiff = self.stats.max(age, (best[0] - self.prediction) % 3, best[1])\n        return (bestdiff[0] + self.prediction) % 3, bestdiff[1]\n\nages = [1000, 100, 10, 5, 2, 1]\n\nclass Iocaine:\n\n    def __init__(self):\n        """Build second-guessers for 50 strategies: 36 history-based strategies,\n             12 simple frequency-based strategies, the constant-move strategy, and\n             the basic random-number-generator strategy.    Also build 6 meta second\n             guessers to evaluate 6 different time horizons on which to score\n             the 50 strategies\' second-guesses."""\n        self.predictors = []\n        self.predict_history = self.predictor((len(ages), 2, 3))\n        self.predict_frequency = self.predictor((len(ages), 2))\n        self.predict_fixed = self.predictor()\n        self.predict_random = self.predictor()\n        self.predict_meta = [Predictor() for a in range(len(ages))]\n        self.stats = [Stats() for i in range(2)]\n        self.histories = [[], [], []]\n\n    def predictor(self, dims=None):\n        """Returns a nested array of predictor objects, of the given dimensions."""\n        if dims: return [self.predictor(dims[1:]) for i in range(dims[0])]\n        self.predictors.append(Predictor())\n        return self.predictors[-1]\n\n    def move(self, them):\n        """The main iocaine "move" function."""\n\n        # histories[0] stores our moves (last one already previously decided);\n        # histories[1] stores their moves (last one just now being supplied to us);\n        # histories[2] stores pairs of our and their last moves.\n        # stats[0] and stats[1] are running counters our recent moves and theirs.\n        if them != -1:\n            self.histories[1].append(them)\n            self.histories[2].append((self.histories[0][-1], them))\n            for watch in range(2):\n                self.stats[watch].add(self.histories[watch][-1], 1)\n\n        # Execute the basic RNG strategy and the fixed-move strategy.\n        rand = random.randrange(3)\n        self.predict_random.addguess(them, rand)\n        self.predict_fixed.addguess(them, 0)\n\n        # Execute the history and frequency stratgies.\n        for a, age in enumerate(ages):\n            # For each time window, there are three ways to recall a similar time:\n            # (0) by history of my moves; (1) their moves; or (2) pairs of moves.\n            # Set "best" to these three timeframes (zero if no matching time).\n            best = [recall(age, hist) for hist in self.histories]\n            for mimic in range(2):\n                # For each similar historical moment, there are two ways to anticipate\n                # the future: by mimicing what their move was; or mimicing what my\n                # move was.    If there were no similar moments, just move randomly.\n                for watch, when in enumerate(best):\n                    if not when: move = rand\n                    else: move = self.histories[mimic][when]\n                    self.predict_history[a][mimic][watch].addguess(them, move)\n                # Also we can anticipate the future by expecting it to be the same\n                # as the most frequent past (either counting their moves or my moves).\n                mostfreq, score = self.stats[mimic].max(age, rand, -1)\n                self.predict_frequency[a][mimic].addguess(them, mostfreq)\n\n        # All the predictors have been updated, but we have not yet scored them\n        # and chosen a winner for this round.    There are several timeframes\n        # on which we can score second-guessing, and we don\'t know timeframe will\n        # do best.    So score all 50 predictors on all 6 timeframes, and record\n        # the best 6 predictions in meta predictors, one for each timeframe.\n        for meta, age in enumerate(ages):\n            best = (-1, -1)\n            for predictor in self.predictors:\n                best = predictor.bestguess(age, best)\n            self.predict_meta[meta].addguess(them, best[0])\n\n        # Finally choose the best meta prediction from the final six, scoring\n        # these against each other on the whole-game timeframe. \n        best = (-1, -1)\n        for meta in range(len(ages)):\n            best = self.predict_meta[meta].bestguess(len(self.histories[0]) , best) \n\n        # We\'ve picked a next move.    Record our move in histories[0] for next time.\n        self.histories[0].append(best[0])\n\n        # And return it.\n        return best[0]\n\niocaine = None\n\ndef iocaine_agent(observation, configuration):\n    global iocaine\n    if observation.step == 0:\n        iocaine = Iocaine()\n        act = iocaine.move(-1)\n    else:\n        act = iocaine.move(observation.lastOpponentAction)\n        \n    return act\n')
    __stickytape_write_module('markov.py', "\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)\n")
    __stickytape_write_module('tree.py', "\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    features = np.append(features, rollouts['steps'])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    \n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action\n")
    __stickytape_write_module('rfind.py', '\nimport random\nhist = []  # history of your moves\ndict_last = {}\nmax_dict_key = 10\nlast_move = 0\n\n\ndef beat(x):\n    return (x + 1) % 3\n\n\ndef predict():\n    global dict_last\n    global max_dict_key\n    for i in reversed(range(min(len(hist), max_dict_key))):\n        t = tuple(hist[-i:])\n        if t in dict_last:\n            return dict_last[t]\n    return random.choice([0, 1, 2])\n\n\ndef update(move, op_move):\n    global hist\n    global dict_last\n    global max_dict_key\n    hist.append(move)\n    for i in reversed(range(min(len(hist), max_dict_key))):\n        t = tuple(hist[-i:])\n        dict_last[t] = op_move\n\n\ndef run(observation, configuration):\n    global last_move\n    if observation.step == 0:\n        last_move = random.choice([0, 1, 2])\n        return last_move\n    update(last_move, observation.lastOpponentAction)\n    move = beat(predict())\n\n    return move\n')
    __stickytape_write_module('statpred.py', '\nimport random\nimport pydash\nfrom collections import Counter\n\n# Create a small amount of starting history\nhistory = {\n    "guess":      [0,1,2],\n    "prediction": [0,1,2],\n    "expected":   [0,1,2],\n    "action":     [0,1,2],\n    "opponent":   [0,1],\n}\ndef statistical_prediction_agent(observation, configuration):    \n    global history\n    actions         = list(range(configuration.signs))  # [0,1,2]\n    last_action     = history[\'action\'][-1]\n    opponent_action = observation.lastOpponentAction if observation.step > 0 else 2\n    \n    history[\'opponent\'].append(opponent_action)\n\n    # Make weighted random guess based on the complete move history, weighted towards relative moves based on our last action \n    move_frequency       = Counter(history[\'opponent\'])\n    response_frequency   = Counter(zip(history[\'action\'], history[\'opponent\'])) \n    move_weights         = [ move_frequency.get(n,1) + response_frequency.get((last_action,n),1) for n in range(configuration.signs) ] \n    guess                = random.choices( population=actions, weights=move_weights, k=1 )[0]\n    \n    # Compare our guess to how our opponent actually played\n    guess_frequency      = Counter(zip(history[\'guess\'], history[\'opponent\']))\n    guess_weights        = [ guess_frequency.get((guess,n),1) for n in range(configuration.signs) ]\n    prediction           = random.choices( population=actions, weights=guess_weights, k=1 )[0]\n\n    # Repeat, but based on how many times our prediction was correct\n    prediction_frequency = Counter(zip(history[\'prediction\'], history[\'opponent\']))\n    prediction_weights   = [ prediction_frequency.get((prediction,n),1) for n in range(configuration.signs) ]\n    expected             = random.choices( population=actions, weights=prediction_weights, k=1 )[0]\n\n    # Play the +1 counter move\n    action = (expected + 1) % configuration.signs\n    \n    # Persist state\n    history[\'guess\'].append(guess)\n    history[\'prediction\'].append(prediction)\n    history[\'expected\'].append(expected)\n    history[\'action\'].append(action)\n\n    # Print debug information\n    print(\'opponent_action                = \', opponent_action)\n    print(\'move_weights,       guess      = \', move_weights, guess)\n    print(\'guess_weights,      prediction = \', guess_weights, prediction)\n    print(\'prediction_weights, expected   = \', prediction_weights, expected)\n    print(\'action                         = \', action)\n    print()\n    \n    return action\n')
    __stickytape_write_module('stotransition.py', 'import numpy as np\nimport json\n#import torch\n\nmatrix = np.ones((3,3,3)) * (1/3) #so we can choose object based on what we chose and what the opponent chose transition matrix\nmatrix_freq = np.ones((3,3,3)) #frequency matrix\nprev_me = 0\nprev_op = 0\n#print(state_dict)\n\ndef copy_opponent_agent (observation, configuration):\n    \n    global prev_me, prev_op, matrix, matrix_freq\n        \n    if observation.step > 0:\n        #return (observation.lastOpponentAction + 1)%3\n        #prev_op = observation.lastOpponentAction #we store the last action of the opponent\n        \n        #from step > 1 we can update matrix because we know what we chose and what it chose\n        if observation.step > 1:\n            matrix_freq[prev_op, prev_me, observation.lastOpponentAction] += 1\n            matrix[prev_op, prev_me, :] = matrix_freq[prev_op, prev_me, :] / np.sum(matrix_freq[prev_op, prev_me, :]) \n            \n        \n        prev_op = observation.lastOpponentAction #we store the last action of the opponent  \n        \n        #choose the optimal choice based on the transition matrix\n        #choosing stochastically\n        prev_me = (np.random.choice(3, p=matrix[prev_op, prev_me, :]) + 1) % 3\n        \n        #print(matrix) \n        \n        state_dict = {"transition tensor" : matrix.tolist()}\n        with open(\'transition_matrix.json\', \'a\') as outfile:\n            json.dump(state_dict, outfile)\n            outfile.write("\\n")\n        \n        return prev_me\n        \n              \n    else:\n        #prev_me = np.random.randint(0,3)\n        state_dict = {"transition tensor" : matrix.tolist()}\n        with open(\'transition_matrix.json\', \'w\') as outfile:\n            json.dump(state_dict, outfile)\n            outfile.write("\\n")\n        prev_me = (np.random.choice(3, p=matrix[prev_op, prev_me, :]) + 1)%3\n        return prev_me\n        #json.dump(matrix_freq, outfile)\n')
    import numpy as np
    # Import various methods.
    import beta
    import bumblepuppy
    import dllu
    import greenberg
    import iou
    import memory
    import randomforest
    import transition
    import iocaine
    import markov
    import tree
    import rfind
    import statpred
    import stotransition
    
    # Use Proportional Representation to find most favorable move.
    def proportionalRepresentation(array_of_moves, array_of_won_rounds, array_of_lost_rounds, step):
        def filterWinrates(moves, win_rounds, lose_rounds):
            indexes = []
            rewards = win_rounds - lose_rounds
            for i in range(len(rewards)):
                # If one move's winrate is below 25%, remove it from the calculation.
                if rewards[i] < 0:
                    indexes.append(i)
            moves = np.delete(moves, indexes)
            rewards = np.delete(rewards, indexes)
            return moves, rewards
    
        # Filter out the methods that has very low win rate.
        array_of_moves, array_of_rewards = filterWinrates(array_of_moves,
                                                          array_of_won_rounds,
                                                          array_of_lost_rounds)
    
        # If no method has threshold winrate, return a random move.
        if len(array_of_rewards) == 0:
            return np.random.randint(3)
    
        move_dict = {0: np.array([]), 1: np.array([]), 2:np.array([])}
        
        for index, element in enumerate(array_of_moves):
            move_dict[element] = np.append(move_dict[element], array_of_rewards[index])
        for move in move_dict:
            if len(move_dict[move]) == 0:
                move_dict[move] = 0
            else:
                move_dict[move] = sum(move_dict[move]) / len(move_dict[move])
    
        best_move = 0
        best_rating = -1
        for move in move_dict:
            if move_dict[move] > best_rating:
                best_move = move
                best_rating = move_dict[move]
        return best_move
    
    
    def updateWonLostRounds(array_of_moves, opponent_move, array_won_rounds):
        current_win_round = np.array([0] * len(array_of_moves))
        current_lose_round = np.array([0] * len(array_of_moves))
        for index, move in enumerate(array_of_moves):
            if move == (opponent_move + 1) % 3:
                current_win_round[index] = 1
            if move == (opponent_move + 2) % 3:
                current_lose_round[index] = 1
        return current_win_round, current_lose_round
    
    
    main_step = 0
    main_won_rounds = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
    main_lost_rounds = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
    main_method_steps = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
    
    def main(observation, configuration):
        global main_step
        global main_won_rounds
        global main_method_steps
        global main_lost_rounds
        # If not first step, update # of won round first.
        if main_step > 0:
            current_win_round, current_lose_round = updateWonLostRounds(main_method_steps, 
                                                                        int(observation.lastOpponentAction), 
                                                                        main_won_rounds)
            main_won_rounds = main_won_rounds + current_win_round
            main_lost_rounds = main_lost_rounds + current_lose_round
    
        # Compute moves generated by each method.
        step_random_forest = randomforest.random_forest_random(observation, configuration)
        step_transition = transition.transition_agent(observation, configuration)
        step_multi_armed = beta.multi_armed_bandit_agent (observation, configuration)
        step_dllu1 = dllu.run(observation, configuration)
        step_greenberg = greenberg.greenberg_agent(observation, configuration)
        step_IOU = iou.agent(observation, configuration)
        step_memory = memory.my_agent(observation, configuration)
        step_bumblepuppy = bumblepuppy.run(observation, configuration)
        step_iocaine = iocaine.iocaine_agent(observation, configuration)
        step_markov = markov.markov_agent(observation, configuration)
        step_rfind = rfind.run(observation, configuration)
        step_statpred = statpred.statistical_prediction_agent(observation, configuration)
        step_stotransition = stotransition.copy_opponent_agent(observation, configuration)
        step_tree = tree.agent(observation, configuration)
    
        # Have all the moves in a np array.
        method_steps = np.array([step_random_forest, step_transition, step_multi_armed, step_dllu1, 
                                      step_greenberg, step_IOU, step_memory, step_bumblepuppy, step_iocaine, 
                                      step_markov, step_rfind, step_statpred, step_stotransition, step_tree])
    
        counter_moves = (method_steps + 1) % 3
    
        main_method_steps = np.concatenate((method_steps, counter_moves))
    
        # If not first step, find current optimized step.
        if main_step > 0:
            selected_step = proportionalRepresentation(main_method_steps, main_won_rounds, 
                                                       main_lost_rounds, main_step)
            main_step += 1
            return selected_step
        # Otherwise, return random number.
        else:
            main_step += 1
            return np.random.randint(3)